{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics and Probability for Data Science\n",
    "\n",
    "Understanding statistics and probability is fundamental for data science, machine learning, and NLP. This notebook covers essential statistical concepts that you'll encounter in data analysis and modeling.\n",
    "\n",
    "## Why Statistics Matter:\n",
    "- **Data Analysis**: Describe and summarize datasets\n",
    "- **Machine Learning**: Understand model performance and uncertainty\n",
    "- **NLP**: Analyze text patterns, word frequencies, language models\n",
    "- **Hypothesis Testing**: Make data-driven decisions\n",
    "- **Feature Engineering**: Create meaningful variables from raw data\n",
    "\n",
    "## Topics Covered:\n",
    "- Descriptive statistics\n",
    "- Probability distributions\n",
    "- Central Limit Theorem\n",
    "- Correlation and causation\n",
    "- Hypothesis testing\n",
    "- Confidence intervals\n",
    "- Practical applications with Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import norm, binom, poisson, chi2_contingency\n",
    "import warnings\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample dataset: student exam scores\n",
    "n_students = 1000\n",
    "exam_scores = np.random.normal(75, 12, n_students)  # mean=75, std=12\n",
    "exam_scores = np.clip(exam_scores, 0, 100)  # Ensure scores are between 0-100\n",
    "\n",
    "print(\"ðŸ“Š Descriptive Statistics for Exam Scores:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Measures of Central Tendency\n",
    "mean_score = np.mean(exam_scores)\n",
    "median_score = np.median(exam_scores)\n",
    "mode_result = stats.mode(exam_scores.round())\n",
    "mode_score = mode_result.mode[0] if len(mode_result.mode) > 0 else \"N/A\"\n",
    "\n",
    "print(f\"Mean (average): {mean_score:.2f}\")\n",
    "print(f\"Median (middle value): {median_score:.2f}\")\n",
    "print(f\"Mode (most frequent): {mode_score}\")\n",
    "print()\n",
    "\n",
    "# Measures of Spread/Variability\n",
    "std_score = np.std(exam_scores, ddof=1)  # Sample standard deviation\n",
    "var_score = np.var(exam_scores, ddof=1)  # Sample variance\n",
    "range_score = np.max(exam_scores) - np.min(exam_scores)\n",
    "iqr_score = np.percentile(exam_scores, 75) - np.percentile(exam_scores, 25)\n",
    "\n",
    "print(f\"Standard Deviation: {std_score:.2f}\")\n",
    "print(f\"Variance: {var_score:.2f}\")\n",
    "print(f\"Range: {range_score:.2f}\")\n",
    "print(f\"Interquartile Range (IQR): {iqr_score:.2f}\")\n",
    "print()\n",
    "\n",
    "# Percentiles\n",
    "percentiles = [10, 25, 50, 75, 90, 95, 99]\n",
    "print(\"Percentiles:\")\n",
    "for p in percentiles:\n",
    "    value = np.percentile(exam_scores, p)\n",
    "    print(f\"  {p:2d}th percentile: {value:.2f}\")\n",
    "\n",
    "# Skewness and Kurtosis\n",
    "skewness = stats.skew(exam_scores)\n",
    "kurtosis = stats.kurtosis(exam_scores)\n",
    "\n",
    "print(f\"\\nSkewness: {skewness:.3f}\")\n",
    "if skewness > 0.5:\n",
    "    print(\"  â†’ Right-skewed (tail extends to the right)\")\n",
    "elif skewness < -0.5:\n",
    "    print(\"  â†’ Left-skewed (tail extends to the left)\")\n",
    "else:\n",
    "    print(\"  â†’ Approximately symmetric\")\n",
    "\n",
    "print(f\"Kurtosis: {kurtosis:.3f}\")\n",
    "if kurtosis > 0:\n",
    "    print(\"  â†’ Heavy-tailed (more extreme values than normal distribution)\")\n",
    "else:\n",
    "    print(\"  â†’ Light-tailed (fewer extreme values than normal distribution)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the distribution\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "fig.suptitle('Statistical Analysis of Exam Scores', fontsize=16)\n",
    "\n",
    "# Histogram\n",
    "axes[0, 0].hist(exam_scores, bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "axes[0, 0].axvline(mean_score, color='red', linestyle='--', label=f'Mean: {mean_score:.1f}')\n",
    "axes[0, 0].axvline(median_score, color='green', linestyle='--', label=f'Median: {median_score:.1f}')\n",
    "axes[0, 0].set_title('Distribution of Exam Scores')\n",
    "axes[0, 0].set_xlabel('Score')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Box plot\n",
    "axes[0, 1].boxplot(exam_scores, vert=True)\n",
    "axes[0, 1].set_title('Box Plot of Exam Scores')\n",
    "axes[0, 1].set_ylabel('Score')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Q-Q plot (Quantile-Quantile plot)\n",
    "stats.probplot(exam_scores, dist=\"norm\", plot=axes[1, 0])\n",
    "axes[1, 0].set_title('Q-Q Plot (Normal Distribution)')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Cumulative Distribution Function\n",
    "sorted_scores = np.sort(exam_scores)\n",
    "cumulative_prob = np.arange(1, len(sorted_scores) + 1) / len(sorted_scores)\n",
    "axes[1, 1].plot(sorted_scores, cumulative_prob, linewidth=2)\n",
    "axes[1, 1].set_title('Cumulative Distribution Function')\n",
    "axes[1, 1].set_xlabel('Score')\n",
    "axes[1, 1].set_ylabel('Cumulative Probability')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Identify outliers using IQR method\n",
    "Q1 = np.percentile(exam_scores, 25)\n",
    "Q3 = np.percentile(exam_scores, 75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "outliers = exam_scores[(exam_scores < lower_bound) | (exam_scores > upper_bound)]\n",
    "print(f\"\\nðŸ” Outlier Analysis:\")\n",
    "print(f\"Number of outliers: {len(outliers)}\")\n",
    "print(f\"Percentage of outliers: {len(outliers)/len(exam_scores)*100:.2f}%\")\n",
    "if len(outliers) > 0:\n",
    "    print(f\"Outlier range: {outliers.min():.2f} to {outliers.max():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common probability distributions\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "fig.suptitle('Common Probability Distributions', fontsize=16)\n",
    "\n",
    "# 1. Normal Distribution\n",
    "x_norm = np.linspace(-4, 4, 100)\n",
    "y_norm = norm.pdf(x_norm, 0, 1)\n",
    "axes[0, 0].plot(x_norm, y_norm, 'b-', linewidth=2, label='Î¼=0, Ïƒ=1')\n",
    "axes[0, 0].fill_between(x_norm, y_norm, alpha=0.3)\n",
    "axes[0, 0].set_title('Normal Distribution')\n",
    "axes[0, 0].set_xlabel('Value')\n",
    "axes[0, 0].set_ylabel('Probability Density')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Binomial Distribution\n",
    "n, p = 20, 0.3\n",
    "x_binom = np.arange(0, n+1)\n",
    "y_binom = binom.pmf(x_binom, n, p)\n",
    "axes[0, 1].bar(x_binom, y_binom, alpha=0.7, color='orange')\n",
    "axes[0, 1].set_title(f'Binomial Distribution (n={n}, p={p})')\n",
    "axes[0, 1].set_xlabel('Number of Successes')\n",
    "axes[0, 1].set_ylabel('Probability')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Poisson Distribution\n",
    "lam = 3\n",
    "x_poisson = np.arange(0, 15)\n",
    "y_poisson = poisson.pmf(x_poisson, lam)\n",
    "axes[0, 2].bar(x_poisson, y_poisson, alpha=0.7, color='green')\n",
    "axes[0, 2].set_title(f'Poisson Distribution (Î»={lam})')\n",
    "axes[0, 2].set_xlabel('Number of Events')\n",
    "axes[0, 2].set_ylabel('Probability')\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Exponential Distribution\n",
    "x_exp = np.linspace(0, 5, 100)\n",
    "y_exp = stats.expon.pdf(x_exp, scale=1)\n",
    "axes[1, 0].plot(x_exp, y_exp, 'r-', linewidth=2, label='Î»=1')\n",
    "axes[1, 0].fill_between(x_exp, y_exp, alpha=0.3, color='red')\n",
    "axes[1, 0].set_title('Exponential Distribution')\n",
    "axes[1, 0].set_xlabel('Value')\n",
    "axes[1, 0].set_ylabel('Probability Density')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Uniform Distribution\n",
    "x_uniform = np.linspace(-2, 2, 100)\n",
    "y_uniform = stats.uniform.pdf(x_uniform, loc=-1, scale=2)\n",
    "axes[1, 1].plot(x_uniform, y_uniform, 'purple', linewidth=2, label='a=-1, b=1')\n",
    "axes[1, 1].fill_between(x_uniform, y_uniform, alpha=0.3, color='purple')\n",
    "axes[1, 1].set_title('Uniform Distribution')\n",
    "axes[1, 1].set_xlabel('Value')\n",
    "axes[1, 1].set_ylabel('Probability Density')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Chi-square Distribution\n",
    "x_chi2 = np.linspace(0, 15, 100)\n",
    "dfs = [1, 3, 5, 9]\n",
    "colors = ['red', 'blue', 'green', 'orange']\n",
    "for df, color in zip(dfs, colors):\n",
    "    y_chi2 = stats.chi2.pdf(x_chi2, df)\n",
    "    axes[1, 2].plot(x_chi2, y_chi2, color=color, linewidth=2, label=f'df={df}')\n",
    "axes[1, 2].set_title('Chi-square Distribution')\n",
    "axes[1, 2].set_xlabel('Value')\n",
    "axes[1, 2].set_ylabel('Probability Density')\n",
    "axes[1, 2].legend()\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Practical examples of when to use each distribution\n",
    "print(\"ðŸ“Š When to Use Each Distribution:\")\n",
    "print(\"=\" * 50)\n",
    "distribution_uses = {\n",
    "    \"Normal\": \"Heights, test scores, measurement errors, many natural phenomena\",\n",
    "    \"Binomial\": \"Number of successes in fixed trials (coin flips, A/B testing)\",\n",
    "    \"Poisson\": \"Rare events over time (website visits, defects, accidents)\",\n",
    "    \"Exponential\": \"Time between events, survival analysis, reliability\",\n",
    "    \"Uniform\": \"Random number generation, equal probability outcomes\",\n",
    "    \"Chi-square\": \"Goodness of fit tests, independence testing\"\n",
    "}\n",
    "\n",
    "for dist, use_case in distribution_uses.items():\n",
    "    print(f\"{dist:12}: {use_case}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Central Limit Theorem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate Central Limit Theorem\n",
    "def demonstrate_clt(population_dist='uniform', n_samples=1000, sample_sizes=[1, 5, 10, 30]):\n",
    "    \"\"\"\n",
    "    Demonstrate the Central Limit Theorem with different sample sizes.\n",
    "    \"\"\"\n",
    "    # Generate population based on distribution type\n",
    "    if population_dist == 'uniform':\n",
    "        population = np.random.uniform(0, 10, 10000)\n",
    "        dist_name = 'Uniform (0, 10)'\n",
    "    elif population_dist == 'exponential':\n",
    "        population = np.random.exponential(2, 10000)\n",
    "        dist_name = 'Exponential (Î»=0.5)'\n",
    "    else:  # skewed\n",
    "        population = np.random.gamma(2, 2, 10000)\n",
    "        dist_name = 'Gamma (skewed)'\n",
    "    \n",
    "    fig, axes = plt.subplots(2, len(sample_sizes), figsize=(16, 8))\n",
    "    fig.suptitle(f'Central Limit Theorem Demonstration\\nPopulation: {dist_name}', fontsize=14)\n",
    "    \n",
    "    # Show original population\n",
    "    axes[0, 0].hist(population, bins=50, alpha=0.7, color='lightcoral', density=True)\n",
    "    axes[0, 0].set_title('Original Population')\n",
    "    axes[0, 0].set_ylabel('Density')\n",
    "    \n",
    "    sample_means = []\n",
    "    \n",
    "    for i, sample_size in enumerate(sample_sizes):\n",
    "        # Generate sample means\n",
    "        means = []\n",
    "        for _ in range(n_samples):\n",
    "            sample = np.random.choice(population, sample_size, replace=False)\n",
    "            means.append(np.mean(sample))\n",
    "        \n",
    "        sample_means.append(means)\n",
    "        \n",
    "        # Plot histogram of sample means\n",
    "        if i == 0:\n",
    "            axes[0, i].hist(population, bins=50, alpha=0.7, color='lightcoral', density=True)\n",
    "            axes[0, i].set_title('Original Population')\n",
    "        else:\n",
    "            axes[0, i].hist(means, bins=30, alpha=0.7, color='skyblue', density=True)\n",
    "            axes[0, i].set_title(f'Sample Means (n={sample_size})')\n",
    "        \n",
    "        axes[0, i].set_xlabel('Value')\n",
    "        if i == 0:\n",
    "            axes[0, i].set_ylabel('Density')\n",
    "        \n",
    "        # Q-Q plot to check normality\n",
    "        if i == 0:\n",
    "            stats.probplot(population[:1000], dist=\"norm\", plot=axes[1, i])\n",
    "            axes[1, i].set_title('Q-Q Plot: Population')\n",
    "        else:\n",
    "            stats.probplot(means, dist=\"norm\", plot=axes[1, i])\n",
    "            axes[1, i].set_title(f'Q-Q Plot: n={sample_size}')\n",
    "        \n",
    "        axes[1, i].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Statistical analysis\n",
    "    print(f\"\\nðŸ“Š Central Limit Theorem Analysis:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Population mean: {np.mean(population):.3f}\")\n",
    "    print(f\"Population std: {np.std(population):.3f}\")\n",
    "    print()\n",
    "    \n",
    "    for i, sample_size in enumerate(sample_sizes[1:], 1):  # Skip population\n",
    "        means = sample_means[i]\n",
    "        theoretical_std = np.std(population) / np.sqrt(sample_size)\n",
    "        actual_std = np.std(means)\n",
    "        \n",
    "        print(f\"Sample size n={sample_size}:\")\n",
    "        print(f\"  Mean of sample means: {np.mean(means):.3f}\")\n",
    "        print(f\"  Std of sample means: {actual_std:.3f}\")\n",
    "        print(f\"  Theoretical std (Ïƒ/âˆšn): {theoretical_std:.3f}\")\n",
    "        print(f\"  Difference: {abs(actual_std - theoretical_std):.3f}\")\n",
    "        \n",
    "        # Test normality with Shapiro-Wilk test\n",
    "        shapiro_stat, shapiro_p = stats.shapiro(means[:5000])  # Limit sample size for test\n",
    "        print(f\"  Normality test p-value: {shapiro_p:.4f}\")\n",
    "        print(f\"  {'âœ… Approximately normal' if shapiro_p > 0.05 else 'âŒ Not normal'}\")\n",
    "        print()\n",
    "\n",
    "# Demonstrate CLT with uniform distribution\n",
    "demonstrate_clt('uniform')\n",
    "\n",
    "print(\"\\nðŸŽ¯ Key Insights from Central Limit Theorem:\")\n",
    "clt_insights = [\n",
    "    \"Sample means approach normal distribution regardless of population shape\",\n",
    "    \"Larger sample sizes lead to more normal distributions\",\n",
    "    \"Mean of sample means equals population mean\",\n",
    "    \"Standard error decreases as sample size increases (Ïƒ/âˆšn)\",\n",
    "    \"CLT is foundation for confidence intervals and hypothesis testing\"\n",
    "]\n",
    "\n",
    "for insight in clt_insights:\n",
    "    print(f\"â€¢ {insight}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation and Causation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate correlated datasets\n",
    "n = 100\n",
    "\n",
    "# Dataset 1: Strong positive correlation\n",
    "x1 = np.random.normal(0, 1, n)\n",
    "y1 = 2 * x1 + np.random.normal(0, 0.5, n)\n",
    "\n",
    "# Dataset 2: No correlation\n",
    "x2 = np.random.normal(0, 1, n)\n",
    "y2 = np.random.normal(0, 1, n)\n",
    "\n",
    "# Dataset 3: Negative correlation\n",
    "x3 = np.random.normal(0, 1, n)\n",
    "y3 = -1.5 * x3 + np.random.normal(0, 0.8, n)\n",
    "\n",
    "# Dataset 4: Non-linear relationship\n",
    "x4 = np.linspace(-2, 2, n)\n",
    "y4 = x4**2 + np.random.normal(0, 0.3, n)\n",
    "\n",
    "# Calculate correlations\n",
    "corr1 = np.corrcoef(x1, y1)[0, 1]\n",
    "corr2 = np.corrcoef(x2, y2)[0, 1]\n",
    "corr3 = np.corrcoef(x3, y3)[0, 1]\n",
    "corr4 = np.corrcoef(x4, y4)[0, 1]\n",
    "\n",
    "# Plotting\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "fig.suptitle('Correlation Examples', fontsize=16)\n",
    "\n",
    "datasets = [(x1, y1, corr1, 'Strong Positive'), (x2, y2, corr2, 'No Correlation'), \n",
    "           (x3, y3, corr3, 'Negative'), (x4, y4, corr4, 'Non-linear')]\n",
    "\n",
    "positions = [(0,0), (0,1), (1,0), (1,1)]\n",
    "\n",
    "for (x, y, corr, title), (i, j) in zip(datasets, positions):\n",
    "    axes[i, j].scatter(x, y, alpha=0.6)\n",
    "    \n",
    "    # Add trend line for linear relationships\n",
    "    if title != 'Non-linear':\n",
    "        z = np.polyfit(x, y, 1)\n",
    "        p = np.poly1d(z)\n",
    "        axes[i, j].plot(x, p(x), \"r--\", alpha=0.8)\n",
    "    \n",
    "    axes[i, j].set_title(f'{title}\\nCorrelation: {corr:.3f}')\n",
    "    axes[i, j].set_xlabel('X')\n",
    "    axes[i, j].set_ylabel('Y')\n",
    "    axes[i, j].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Correlation interpretation\n",
    "def interpret_correlation(r):\n",
    "    abs_r = abs(r)\n",
    "    if abs_r >= 0.9:\n",
    "        return \"Very strong\"\n",
    "    elif abs_r >= 0.7:\n",
    "        return \"Strong\"\n",
    "    elif abs_r >= 0.5:\n",
    "        return \"Moderate\"\n",
    "    elif abs_r >= 0.3:\n",
    "        return \"Weak\"\n",
    "    else:\n",
    "        return \"Very weak\"\n",
    "\n",
    "print(\"\\nðŸ“Š Correlation Analysis:\")\n",
    "print(\"=\" * 40)\n",
    "for i, (title, corr) in enumerate([(\"Strong Positive\", corr1), (\"No Correlation\", corr2), \n",
    "                                  (\"Negative\", corr3), (\"Non-linear\", corr4)]):\n",
    "    strength = interpret_correlation(corr)\n",
    "    direction = \"positive\" if corr > 0 else \"negative\" if corr < 0 else \"no\"\n",
    "    print(f\"{title:15}: r = {corr:6.3f} ({strength} {direction} correlation)\")\n",
    "\n",
    "print(\"\\nâš ï¸ Important Notes:\")\n",
    "print(\"â€¢ Correlation measures LINEAR relationships only\")\n",
    "print(\"â€¢ Non-linear relationships may have low correlation but strong association\")\n",
    "print(\"â€¢ Correlation â‰  Causation (correlation does not imply causation)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spurious correlation example\n",
    "np.random.seed(123)\n",
    "years = np.arange(2010, 2021)\n",
    "n_years = len(years)\n",
    "\n",
    "# Create two unrelated time series that happen to be correlated\n",
    "# Ice cream sales (increases over time due to warming)\n",
    "ice_cream_sales = 1000 + 50 * np.arange(n_years) + np.random.normal(0, 20, n_years)\n",
    "\n",
    "# Sunglasses sales (also increases over time due to fashion trends)\n",
    "sunglasses_sales = 500 + 30 * np.arange(n_years) + np.random.normal(0, 15, n_years)\n",
    "\n",
    "# Calculate correlation\n",
    "spurious_corr = np.corrcoef(ice_cream_sales, sunglasses_sales)[0, 1]\n",
    "\n",
    "# Plot\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Time series\n",
    "ax1.plot(years, ice_cream_sales, 'o-', label='Ice Cream Sales', color='red')\n",
    "ax1.set_xlabel('Year')\n",
    "ax1.set_ylabel('Ice Cream Sales', color='red')\n",
    "ax1.tick_params(axis='y', labelcolor='red')\n",
    "\n",
    "ax1_twin = ax1.twinx()\n",
    "ax1_twin.plot(years, sunglasses_sales, 's-', label='Sunglasses Sales', color='blue')\n",
    "ax1_twin.set_ylabel('Sunglasses Sales', color='blue')\n",
    "ax1_twin.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "ax1.set_title('Time Series: Both Trending Up')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Scatter plot\n",
    "ax2.scatter(ice_cream_sales, sunglasses_sales, alpha=0.7, color='green')\n",
    "z = np.polyfit(ice_cream_sales, sunglasses_sales, 1)\n",
    "p = np.poly1d(z)\n",
    "ax2.plot(ice_cream_sales, p(ice_cream_sales), \"r--\", alpha=0.8)\n",
    "ax2.set_xlabel('Ice Cream Sales')\n",
    "ax2.set_ylabel('Sunglasses Sales')\n",
    "ax2.set_title(f'Spurious Correlation\\nr = {spurious_corr:.3f}')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# After detrending\n",
    "# Remove linear trend from both series\n",
    "ice_cream_detrended = ice_cream_sales - (1000 + 50 * np.arange(n_years))\n",
    "sunglasses_detrended = sunglasses_sales - (500 + 30 * np.arange(n_years))\n",
    "detrended_corr = np.corrcoef(ice_cream_detrended, sunglasses_detrended)[0, 1]\n",
    "\n",
    "ax3.scatter(ice_cream_detrended, sunglasses_detrended, alpha=0.7, color='purple')\n",
    "ax3.set_xlabel('Ice Cream Sales (detrended)')\n",
    "ax3.set_ylabel('Sunglasses Sales (detrended)')\n",
    "ax3.set_title(f'After Detrending\\nr = {detrended_corr:.3f}')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "ax3.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "ax3.axvline(x=0, color='black', linestyle='-', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ðŸš¨ Spurious Correlation Example:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Original correlation: {spurious_corr:.3f}\")\n",
    "print(f\"After detrending: {detrended_corr:.3f}\")\n",
    "print()\n",
    "print(\"Explanation:\")\n",
    "print(\"â€¢ Both variables increase over time (common trend)\")\n",
    "print(\"â€¢ This creates artificial correlation\")\n",
    "print(\"â€¢ After removing trend, correlation disappears\")\n",
    "print(\"â€¢ Ice cream and sunglasses sales are not causally related\")\n",
    "print(\"â€¢ The correlation is due to a confounding variable (time/season)\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ Correlation vs Causation Guidelines:\")\n",
    "guidelines = [\n",
    "    \"High correlation doesn't prove causation\",\n",
    "    \"Look for confounding variables\",\n",
    "    \"Consider reverse causation\",\n",
    "    \"Use experimental design to establish causation\",\n",
    "    \"Apply domain knowledge and logic\",\n",
    "    \"Beware of spurious correlations in time series data\"\n",
    "]\n",
    "\n",
    "for guideline in guidelines:\n",
    "    print(f\"â€¢ {guideline}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypothesis testing examples\n",
    "\n",
    "def perform_hypothesis_test():\n",
    "    print(\"ðŸ§ª Hypothesis Testing Examples\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Example 1: One-sample t-test\n",
    "    print(\"\\n1ï¸âƒ£ One-Sample T-Test:\")\n",
    "    print(\"Hâ‚€: Î¼ = 100 (population mean is 100)\")\n",
    "    print(\"Hâ‚: Î¼ â‰  100 (population mean is not 100)\")\n",
    "    \n",
    "    # Generate sample data\n",
    "    np.random.seed(42)\n",
    "    sample = np.random.normal(103, 8, 50)  # True mean is 103\n",
    "    \n",
    "    # Perform t-test\n",
    "    t_stat, p_value = stats.ttest_1samp(sample, 100)\n",
    "    \n",
    "    print(f\"Sample mean: {np.mean(sample):.2f}\")\n",
    "    print(f\"Sample size: {len(sample)}\")\n",
    "    print(f\"t-statistic: {t_stat:.3f}\")\n",
    "    print(f\"p-value: {p_value:.4f}\")\n",
    "    \n",
    "    alpha = 0.05\n",
    "    if p_value < alpha:\n",
    "        print(f\"âœ… Reject Hâ‚€ (p < {alpha}): Evidence that mean â‰  100\")\n",
    "    else:\n",
    "        print(f\"âŒ Fail to reject Hâ‚€ (p â‰¥ {alpha}): Insufficient evidence\")\n",
    "    \n",
    "    # Example 2: Two-sample t-test\n",
    "    print(\"\\n2ï¸âƒ£ Two-Sample T-Test:\")\n",
    "    print(\"Hâ‚€: Î¼â‚ = Î¼â‚‚ (no difference between group means)\")\n",
    "    print(\"Hâ‚: Î¼â‚ â‰  Î¼â‚‚ (difference between group means)\")\n",
    "    \n",
    "    # Generate two samples\n",
    "    group1 = np.random.normal(100, 10, 30)  # Control group\n",
    "    group2 = np.random.normal(105, 10, 35)  # Treatment group\n",
    "    \n",
    "    # Perform independent t-test\n",
    "    t_stat2, p_value2 = stats.ttest_ind(group1, group2)\n",
    "    \n",
    "    print(f\"Group 1 mean: {np.mean(group1):.2f} (n={len(group1)})\")\n",
    "    print(f\"Group 2 mean: {np.mean(group2):.2f} (n={len(group2)})\")\n",
    "    print(f\"Difference: {np.mean(group2) - np.mean(group1):.2f}\")\n",
    "    print(f\"t-statistic: {t_stat2:.3f}\")\n",
    "    print(f\"p-value: {p_value2:.4f}\")\n",
    "    \n",
    "    if p_value2 < alpha:\n",
    "        print(f\"âœ… Reject Hâ‚€ (p < {alpha}): Significant difference between groups\")\n",
    "    else:\n",
    "        print(f\"âŒ Fail to reject Hâ‚€ (p â‰¥ {alpha}): No significant difference\")\n",
    "    \n",
    "    # Example 3: Chi-square test of independence\n",
    "    print(\"\\n3ï¸âƒ£ Chi-Square Test of Independence:\")\n",
    "    print(\"Hâ‚€: Variables are independent\")\n",
    "    print(\"Hâ‚: Variables are dependent\")\n",
    "    \n",
    "    # Create contingency table\n",
    "    # Gender vs Preference\n",
    "    contingency_table = np.array([[30, 20, 10],   # Male: A, B, C\n",
    "                                 [20, 35, 25]])   # Female: A, B, C\n",
    "    \n",
    "    chi2_stat, chi2_p, dof, expected = chi2_contingency(contingency_table)\n",
    "    \n",
    "    print(\"Contingency Table (Gender vs Preference):\")\n",
    "    print(\"        A    B    C\")\n",
    "    print(f\"Male   {contingency_table[0, 0]:2d}  {contingency_table[0, 1]:2d}  {contingency_table[0, 2]:2d}\")\n",
    "    print(f\"Female {contingency_table[1, 0]:2d}  {contingency_table[1, 1]:2d}  {contingency_table[1, 2]:2d}\")\n",
    "    print()\n",
    "    print(f\"Chi-square statistic: {chi2_stat:.3f}\")\n",
    "    print(f\"Degrees of freedom: {dof}\")\n",
    "    print(f\"p-value: {chi2_p:.4f}\")\n",
    "    \n",
    "    if chi2_p < alpha:\n",
    "        print(f\"âœ… Reject Hâ‚€ (p < {alpha}): Variables are dependent\")\n",
    "    else:\n",
    "        print(f\"âŒ Fail to reject Hâ‚€ (p â‰¥ {alpha}): Variables are independent\")\n",
    "    \n",
    "    return sample, group1, group2\n",
    "\n",
    "# Perform the tests\n",
    "sample_data, group1_data, group2_data = perform_hypothesis_test()\n",
    "\n",
    "# Visualize the hypothesis tests\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# One-sample test visualization\n",
    "axes[0].hist(sample_data, bins=15, alpha=0.7, color='lightblue', density=True)\n",
    "axes[0].axvline(100, color='red', linestyle='--', linewidth=2, label='Hâ‚€: Î¼ = 100')\n",
    "axes[0].axvline(np.mean(sample_data), color='green', linestyle='-', linewidth=2, label=f'Sample mean = {np.mean(sample_data):.1f}')\n",
    "axes[0].set_title('One-Sample T-Test')\n",
    "axes[0].set_xlabel('Value')\n",
    "axes[0].set_ylabel('Density')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Two-sample test visualization\n",
    "axes[1].hist(group1_data, bins=15, alpha=0.6, color='lightcoral', density=True, label=f'Group 1 (Î¼={np.mean(group1_data):.1f})')\n",
    "axes[1].hist(group2_data, bins=15, alpha=0.6, color='lightgreen', density=True, label=f'Group 2 (Î¼={np.mean(group2_data):.1f})')\n",
    "axes[1].set_title('Two-Sample T-Test')\n",
    "axes[1].set_xlabel('Value')\n",
    "axes[1].set_ylabel('Density')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confidence Intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confidence intervals demonstration\n",
    "def calculate_confidence_intervals(data, confidence_levels=[0.90, 0.95, 0.99]):\n",
    "    \"\"\"\n",
    "    Calculate confidence intervals for different confidence levels.\n",
    "    \"\"\"\n",
    "    n = len(data)\n",
    "    mean = np.mean(data)\n",
    "    std_err = stats.sem(data)  # Standard error of the mean\n",
    "    \n",
    "    print(f\"ðŸ“Š Confidence Intervals (n={n}):\")\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"Sample mean: {mean:.3f}\")\n",
    "    print(f\"Standard error: {std_err:.3f}\")\n",
    "    print()\n",
    "    \n",
    "    intervals = {}\n",
    "    \n",
    "    for confidence in confidence_levels:\n",
    "        # Calculate t-critical value\n",
    "        alpha = 1 - confidence\n",
    "        t_critical = stats.t.ppf(1 - alpha/2, df=n-1)\n",
    "        \n",
    "        # Calculate margin of error\n",
    "        margin_error = t_critical * std_err\n",
    "        \n",
    "        # Calculate confidence interval\n",
    "        ci_lower = mean - margin_error\n",
    "        ci_upper = mean + margin_error\n",
    "        \n",
    "        intervals[confidence] = (ci_lower, ci_upper)\n",
    "        \n",
    "        print(f\"{confidence*100:4.0f}% CI: [{ci_lower:7.3f}, {ci_upper:7.3f}] (width: {ci_upper - ci_lower:.3f})\")\n",
    "    \n",
    "    return intervals\n",
    "\n",
    "# Generate sample data\n",
    "np.random.seed(42)\n",
    "sample_size = 50\n",
    "true_mean = 25\n",
    "sample = np.random.normal(true_mean, 5, sample_size)\n",
    "\n",
    "# Calculate confidence intervals\n",
    "ci_intervals = calculate_confidence_intervals(sample)\n",
    "\n",
    "# Visualize confidence intervals\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Plot 1: Sample distribution with confidence intervals\n",
    "ax1.hist(sample, bins=15, alpha=0.7, color='lightblue', density=True)\n",
    "ax1.axvline(true_mean, color='red', linestyle='--', linewidth=2, label=f'True mean = {true_mean}')\n",
    "ax1.axvline(np.mean(sample), color='green', linestyle='-', linewidth=2, label=f'Sample mean = {np.mean(sample):.2f}')\n",
    "\n",
    "# Add confidence intervals\n",
    "colors = ['orange', 'purple', 'brown']\n",
    "confidences = [0.90, 0.95, 0.99]\n",
    "y_positions = [0.12, 0.10, 0.08]\n",
    "\n",
    "for i, (conf, color, y_pos) in enumerate(zip(confidences, colors, y_positions)):\n",
    "    ci_lower, ci_upper = ci_intervals[conf]\n",
    "    ax1.plot([ci_lower, ci_upper], [y_pos, y_pos], color=color, linewidth=4, \n",
    "             label=f'{conf*100:.0f}% CI')\n",
    "    ax1.plot([ci_lower, ci_lower], [y_pos-0.005, y_pos+0.005], color=color, linewidth=2)\n",
    "    ax1.plot([ci_upper, ci_upper], [y_pos-0.005, y_pos+0.005], color=color, linewidth=2)\n",
    "\n",
    "ax1.set_title('Sample Distribution with Confidence Intervals')\n",
    "ax1.set_xlabel('Value')\n",
    "ax1.set_ylabel('Density')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: CI simulation - show coverage probability\n",
    "def simulate_ci_coverage(true_mean, std_dev, sample_size, n_simulations=1000, confidence=0.95):\n",
    "    \"\"\"\n",
    "    Simulate confidence interval coverage.\n",
    "    \"\"\"\n",
    "    coverage_count = 0\n",
    "    ci_lowers = []\n",
    "    ci_uppers = []\n",
    "    sample_means = []\n",
    "    \n",
    "    alpha = 1 - confidence\n",
    "    t_critical = stats.t.ppf(1 - alpha/2, df=sample_size-1)\n",
    "    \n",
    "    for _ in range(n_simulations):\n",
    "        # Generate random sample\n",
    "        sample = np.random.normal(true_mean, std_dev, sample_size)\n",
    "        sample_mean = np.mean(sample)\n",
    "        std_err = stats.sem(sample)\n",
    "        \n",
    "        # Calculate CI\n",
    "        margin_error = t_critical * std_err\n",
    "        ci_lower = sample_mean - margin_error\n",
    "        ci_upper = sample_mean + margin_error\n",
    "        \n",
    "        # Check if CI contains true mean\n",
    "        if ci_lower <= true_mean <= ci_upper:\n",
    "            coverage_count += 1\n",
    "        \n",
    "        ci_lowers.append(ci_lower)\n",
    "        ci_uppers.append(ci_upper)\n",
    "        sample_means.append(sample_mean)\n",
    "    \n",
    "    coverage_probability = coverage_count / n_simulations\n",
    "    return coverage_probability, ci_lowers, ci_uppers, sample_means\n",
    "\n",
    "# Run simulation\n",
    "coverage_prob, ci_lowers, ci_uppers, sample_means = simulate_ci_coverage(true_mean, 5, sample_size, 100, 0.95)\n",
    "\n",
    "# Plot first 50 confidence intervals\n",
    "n_show = 50\n",
    "for i in range(n_show):\n",
    "    color = 'green' if ci_lowers[i] <= true_mean <= ci_uppers[i] else 'red'\n",
    "    ax2.plot([ci_lowers[i], ci_uppers[i]], [i, i], color=color, alpha=0.7, linewidth=2)\n",
    "    ax2.plot(sample_means[i], i, 'o', color='blue', markersize=3, alpha=0.7)\n",
    "\n",
    "ax2.axvline(true_mean, color='black', linestyle='--', linewidth=2, label=f'True mean = {true_mean}')\n",
    "ax2.set_title(f'95% Confidence Intervals\\nCoverage: {coverage_prob*100:.1f}% (Expected: 95%)')\n",
    "ax2.set_xlabel('Value')\n",
    "ax2.set_ylabel('Sample Number')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Confidence Interval Interpretation:\")\n",
    "print(\"=\" * 50)\n",
    "interpretations = [\n",
    "    \"95% CI means: If we repeated this study many times, 95% of the\",\n",
    "    \"confidence intervals would contain the true population mean\",\n",
    "    \"\",\n",
    "    \"Common misconceptions:\",\n",
    "    \"âŒ 'There's a 95% chance the true mean is in this interval'\",\n",
    "    \"âœ… 'This interval was created by a method that captures the\",\n",
    "    \"   true mean 95% of the time'\",\n",
    "    \"\",\n",
    "    \"Factors affecting CI width:\",\n",
    "    \"â€¢ Higher confidence level â†’ Wider interval\",\n",
    "    \"â€¢ Larger sample size â†’ Narrower interval\",\n",
    "    \"â€¢ Higher variability â†’ Wider interval\"\n",
    "]\n",
    "\n",
    "for interpretation in interpretations:\n",
    "    print(interpretation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### Essential Statistical Concepts:\n",
    "\n",
    "1. **Descriptive Statistics**:\n",
    "   - **Central Tendency**: Mean, median, mode\n",
    "   - **Variability**: Standard deviation, variance, IQR\n",
    "   - **Shape**: Skewness, kurtosis\n",
    "   - **Position**: Percentiles, quartiles\n",
    "\n",
    "2. **Probability Distributions**:\n",
    "   - **Normal**: Most common, bell-shaped\n",
    "   - **Binomial**: Fixed number of trials, binary outcomes\n",
    "   - **Poisson**: Rare events over time/space\n",
    "   - **Exponential**: Time between events\n",
    "\n",
    "3. **Central Limit Theorem**:\n",
    "   - Sample means approach normal distribution\n",
    "   - Foundation for inference\n",
    "   - Standard error = Ïƒ/âˆšn\n",
    "\n",
    "4. **Correlation vs Causation**:\n",
    "   - Correlation measures linear association\n",
    "   - High correlation â‰  causation\n",
    "   - Watch for confounding variables\n",
    "   - Beware spurious correlations\n",
    "\n",
    "5. **Hypothesis Testing**:\n",
    "   - Set null (Hâ‚€) and alternative (Hâ‚) hypotheses\n",
    "   - Choose significance level (Î±)\n",
    "   - Calculate test statistic and p-value\n",
    "   - Make decision based on p-value\n",
    "\n",
    "6. **Confidence Intervals**:\n",
    "   - Provide range of plausible values\n",
    "   - Interpretation is about the method, not the specific interval\n",
    "   - Width depends on confidence level, sample size, and variability\n",
    "\n",
    "### For Data Science Applications:\n",
    "\n",
    "- **Exploratory Data Analysis**: Use descriptive statistics to understand data\n",
    "- **Feature Engineering**: Apply statistical transformations\n",
    "- **Model Validation**: Use statistical tests to compare models\n",
    "- **A/B Testing**: Apply hypothesis testing to business decisions\n",
    "- **Uncertainty Quantification**: Use confidence intervals for predictions\n",
    "\n",
    "### For NLP Applications:\n",
    "\n",
    "- **Text Analysis**: Word frequency distributions, n-gram statistics\n",
    "- **Language Models**: Probability distributions over words/sequences\n",
    "- **Evaluation**: Statistical significance of model improvements\n",
    "- **Sampling**: Confidence intervals for accuracy metrics\n",
    "\n",
    "## Practice Exercises\n",
    "\n",
    "1. **Analyze a real dataset**: Calculate all descriptive statistics and create visualizations\n",
    "2. **A/B Test simulation**: Design and analyze an experiment with statistical tests\n",
    "3. **Correlation analysis**: Find and explain spurious correlations in time series data\n",
    "4. **Distribution fitting**: Identify which probability distribution best fits your data\n",
    "5. **Power analysis**: Calculate required sample sizes for detecting effects\n",
    "6. **Bootstrap confidence intervals**: Use resampling methods for CI estimation\n",
    "7. **Multiple testing correction**: Handle multiple hypothesis testing problems\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Build on these foundations to:\n",
    "- **Advanced statistics**: Regression, ANOVA, time series analysis\n",
    "- **Machine learning**: Understanding model assumptions and validation\n",
    "- **Bayesian statistics**: Alternative approach to inference\n",
    "- **Experimental design**: Planning studies for causal inference\n",
    "\n",
    "Statistics is the foundation that allows you to make reliable conclusions from data!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}