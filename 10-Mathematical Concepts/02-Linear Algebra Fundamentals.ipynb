{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Algebra Fundamentals for Data Science\n",
    "\n",
    "Linear algebra is the mathematical foundation of machine learning, data science, and NLP. This notebook covers essential linear algebra concepts with practical Python implementations.\n",
    "\n",
    "## Why Linear Algebra Matters:\n",
    "- **Machine Learning**: Neural networks, PCA, regression, SVD\n",
    "- **NLP**: Word embeddings, transformers, document similarity\n",
    "- **Computer Vision**: Image processing, convolutions, transformations\n",
    "- **Data Science**: Dimensionality reduction, clustering, optimization\n",
    "- **Deep Learning**: All operations are essentially matrix multiplications\n",
    "\n",
    "## Topics Covered:\n",
    "- Vectors and vector operations\n",
    "- Matrices and matrix operations\n",
    "- Systems of linear equations\n",
    "- Eigenvalues and eigenvectors\n",
    "- Matrix decomposition (SVD, PCA)\n",
    "- Applications in ML/NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import make_blobs\n",
    "import warnings\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectors and Vector Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector basics\n",
    "print(\"ðŸ“ Vector Operations\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Create vectors\n",
    "v1 = np.array([3, 4])\n",
    "v2 = np.array([1, 2])\n",
    "v3 = np.array([2, -1, 3])  # 3D vector\n",
    "\n",
    "print(f\"Vector v1: {v1}\")\n",
    "print(f\"Vector v2: {v2}\")\n",
    "print(f\"Vector v3: {v3}\")\n",
    "print()\n",
    "\n",
    "# Vector operations\n",
    "print(\"Basic Operations:\")\n",
    "print(f\"v1 + v2 = {v1 + v2}\")\n",
    "print(f\"v1 - v2 = {v1 - v2}\")\n",
    "print(f\"3 * v1 = {3 * v1}\")\n",
    "print()\n",
    "\n",
    "# Vector norms (magnitude)\n",
    "print(\"Vector Norms:\")\n",
    "l2_norm_v1 = np.linalg.norm(v1)  # L2 norm (Euclidean)\n",
    "l1_norm_v1 = np.linalg.norm(v1, ord=1)  # L1 norm (Manhattan)\n",
    "linf_norm_v1 = np.linalg.norm(v1, ord=np.inf)  # Lâˆž norm (Maximum)\n",
    "\n",
    "print(f\"||v1||â‚‚ (L2 norm): {l2_norm_v1:.3f}\")\n",
    "print(f\"||v1||â‚ (L1 norm): {l1_norm_v1:.3f}\")\n",
    "print(f\"||v1||âˆž (Lâˆž norm): {linf_norm_v1:.3f}\")\n",
    "print()\n",
    "\n",
    "# Dot product\n",
    "dot_product = np.dot(v1, v2)\n",
    "print(f\"Dot product v1 Â· v2 = {dot_product}\")\n",
    "\n",
    "# Alternative ways to compute dot product\n",
    "print(f\"Using @ operator: v1 @ v2 = {v1 @ v2}\")\n",
    "print(f\"Manual calculation: {v1[0]*v2[0] + v1[1]*v2[1]}\")\n",
    "print()\n",
    "\n",
    "# Angle between vectors\n",
    "cos_angle = dot_product / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "angle_rad = np.arccos(cos_angle)\n",
    "angle_deg = np.degrees(angle_rad)\n",
    "\n",
    "print(f\"Angle between v1 and v2: {angle_deg:.1f}Â° ({angle_rad:.3f} radians)\")\n",
    "print(f\"Cosine of angle: {cos_angle:.3f}\")\n",
    "\n",
    "# Unit vectors (normalization)\n",
    "v1_unit = v1 / np.linalg.norm(v1)\n",
    "print(f\"\\nUnit vector of v1: {v1_unit}\")\n",
    "print(f\"Norm of unit vector: {np.linalg.norm(v1_unit):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize vectors\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Plot 1: Basic vectors\n",
    "origin = [0, 0]\n",
    "axes[0].quiver(*origin, v1[0], v1[1], angles='xy', scale_units='xy', scale=1, color='red', width=0.005, label='v1')\n",
    "axes[0].quiver(*origin, v2[0], v2[1], angles='xy', scale_units='xy', scale=1, color='blue', width=0.005, label='v2')\n",
    "axes[0].quiver(*origin, (v1+v2)[0], (v1+v2)[1], angles='xy', scale_units='xy', scale=1, color='green', width=0.005, label='v1+v2')\n",
    "\n",
    "# Add vector labels\n",
    "axes[0].text(v1[0]+0.1, v1[1]+0.1, 'v1', fontsize=12, color='red')\n",
    "axes[0].text(v2[0]+0.1, v2[1]+0.1, 'v2', fontsize=12, color='blue')\n",
    "axes[0].text((v1+v2)[0]+0.1, (v1+v2)[1]+0.1, 'v1+v2', fontsize=12, color='green')\n",
    "\n",
    "axes[0].set_xlim(-1, 6)\n",
    "axes[0].set_ylim(-1, 7)\n",
    "axes[0].set_aspect('equal')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_title('Vector Addition')\n",
    "axes[0].set_xlabel('x')\n",
    "axes[0].set_ylabel('y')\n",
    "\n",
    "# Plot 2: Dot product geometric interpretation\n",
    "# Project v2 onto v1\n",
    "v1_unit = v1 / np.linalg.norm(v1)\n",
    "projection_length = np.dot(v2, v1_unit)\n",
    "projection = projection_length * v1_unit\n",
    "\n",
    "axes[1].quiver(*origin, v1[0], v1[1], angles='xy', scale_units='xy', scale=1, color='red', width=0.005)\n",
    "axes[1].quiver(*origin, v2[0], v2[1], angles='xy', scale_units='xy', scale=1, color='blue', width=0.005)\n",
    "axes[1].quiver(*origin, projection[0], projection[1], angles='xy', scale_units='xy', scale=1, color='purple', width=0.005)\n",
    "\n",
    "# Draw projection line\n",
    "axes[1].plot([v2[0], projection[0]], [v2[1], projection[1]], 'k--', alpha=0.5)\n",
    "\n",
    "axes[1].text(v1[0]+0.1, v1[1]+0.1, 'v1', fontsize=12, color='red')\n",
    "axes[1].text(v2[0]+0.1, v2[1]+0.1, 'v2', fontsize=12, color='blue')\n",
    "axes[1].text(projection[0]+0.1, projection[1]-0.3, 'proj', fontsize=12, color='purple')\n",
    "\n",
    "axes[1].set_xlim(-1, 4)\n",
    "axes[1].set_ylim(-1, 5)\n",
    "axes[1].set_aspect('equal')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_title(f'Dot Product = {dot_product}\\n(Projection of v2 onto v1)')\n",
    "axes[1].set_xlabel('x')\n",
    "axes[1].set_ylabel('y')\n",
    "\n",
    "# Plot 3: Different norms visualization\n",
    "circle_angles = np.linspace(0, 2*np.pi, 100)\n",
    "\n",
    "# L2 norm (unit circle)\n",
    "l2_circle_x = np.cos(circle_angles)\n",
    "l2_circle_y = np.sin(circle_angles)\n",
    "axes[2].plot(l2_circle_x, l2_circle_y, 'b-', label='L2 norm (||v||â‚‚ = 1)', linewidth=2)\n",
    "\n",
    "# L1 norm (diamond)\n",
    "l1_diamond_x = [1, 0, -1, 0, 1]\n",
    "l1_diamond_y = [0, 1, 0, -1, 0]\n",
    "axes[2].plot(l1_diamond_x, l1_diamond_y, 'r-', label='L1 norm (||v||â‚ = 1)', linewidth=2)\n",
    "\n",
    "# Lâˆž norm (square)\n",
    "linf_square_x = [1, 1, -1, -1, 1]\n",
    "linf_square_y = [1, -1, -1, 1, 1]\n",
    "axes[2].plot(linf_square_x, linf_square_y, 'g-', label='Lâˆž norm (||v||âˆž = 1)', linewidth=2)\n",
    "\n",
    "axes[2].set_xlim(-1.5, 1.5)\n",
    "axes[2].set_ylim(-1.5, 1.5)\n",
    "axes[2].set_aspect('equal')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "axes[2].set_title('Unit Balls for Different Norms')\n",
    "axes[2].set_xlabel('x')\n",
    "axes[2].set_ylabel('y')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸŽ¯ Key Vector Concepts:\")\n",
    "vector_concepts = [\n",
    "    \"Vectors represent both magnitude and direction\",\n",
    "    \"Dot product measures similarity/projection\",\n",
    "    \"Unit vectors have magnitude 1 (normalized)\",\n",
    "    \"Different norms capture different notions of 'size'\",\n",
    "    \"Orthogonal vectors have dot product = 0\"\n",
    "]\n",
    "\n",
    "for concept in vector_concepts:\n",
    "    print(f\"â€¢ {concept}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrices and Matrix Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix basics\n",
    "print(\"ðŸ”² Matrix Operations\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Create matrices\n",
    "A = np.array([[1, 2, 3],\n",
    "              [4, 5, 6]])\n",
    "\n",
    "B = np.array([[7, 8],\n",
    "              [9, 10],\n",
    "              [11, 12]])\n",
    "\n",
    "C = np.array([[1, 2],\n",
    "              [3, 4]])\n",
    "\n",
    "print(\"Matrix A (2Ã—3):\")\n",
    "print(A)\n",
    "print(f\"Shape: {A.shape}\")\n",
    "print()\n",
    "\n",
    "print(\"Matrix B (3Ã—2):\")\n",
    "print(B)\n",
    "print(f\"Shape: {B.shape}\")\n",
    "print()\n",
    "\n",
    "print(\"Matrix C (2Ã—2):\")\n",
    "print(C)\n",
    "print(f\"Shape: {C.shape}\")\n",
    "print()\n",
    "\n",
    "# Matrix multiplication\n",
    "print(\"Matrix Multiplication:\")\n",
    "AB = A @ B  # or np.dot(A, B)\n",
    "print(f\"A @ B (2Ã—3) @ (3Ã—2) = (2Ã—2):\")\n",
    "print(AB)\n",
    "print()\n",
    "\n",
    "# Element-wise operations\n",
    "print(\"Element-wise Operations:\")\n",
    "print(f\"C + C:\")\n",
    "print(C + C)\n",
    "print(f\"C * C (element-wise):\")\n",
    "print(C * C)\n",
    "print(f\"C @ C (matrix multiplication):\")\n",
    "print(C @ C)\n",
    "print()\n",
    "\n",
    "# Transpose\n",
    "print(\"Matrix Transpose:\")\n",
    "print(f\"A:\")\n",
    "print(A)\n",
    "print(f\"A.T (transpose):\")\n",
    "print(A.T)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Special matrices\n",
    "print(\"ðŸ”· Special Matrices\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Identity matrix\n",
    "I = np.eye(3)\n",
    "print(\"Identity Matrix (3Ã—3):\")\n",
    "print(I)\n",
    "print()\n",
    "\n",
    "# Zero matrix\n",
    "Z = np.zeros((2, 3))\n",
    "print(\"Zero Matrix (2Ã—3):\")\n",
    "print(Z)\n",
    "print()\n",
    "\n",
    "# Ones matrix\n",
    "O = np.ones((3, 2))\n",
    "print(\"Ones Matrix (3Ã—2):\")\n",
    "print(O)\n",
    "print()\n",
    "\n",
    "# Random matrix\n",
    "R = np.random.randn(3, 3)\n",
    "print(\"Random Matrix (3Ã—3):\")\n",
    "print(R.round(3))\n",
    "print()\n",
    "\n",
    "# Diagonal matrix\n",
    "D = np.diag([1, 2, 3, 4])\n",
    "print(\"Diagonal Matrix:\")\n",
    "print(D)\n",
    "print()\n",
    "\n",
    "# Matrix properties\n",
    "test_matrix = np.array([[2, 1], [1, 2]])\n",
    "print(f\"Test Matrix:\")\n",
    "print(test_matrix)\n",
    "print()\n",
    "\n",
    "# Determinant\n",
    "det = np.linalg.det(test_matrix)\n",
    "print(f\"Determinant: {det:.3f}\")\n",
    "\n",
    "# Trace (sum of diagonal elements)\n",
    "trace = np.trace(test_matrix)\n",
    "print(f\"Trace: {trace}\")\n",
    "\n",
    "# Rank\n",
    "rank = np.linalg.matrix_rank(test_matrix)\n",
    "print(f\"Rank: {rank}\")\n",
    "print()\n",
    "\n",
    "# Matrix inverse (if it exists)\n",
    "if det != 0:\n",
    "    inv_matrix = np.linalg.inv(test_matrix)\n",
    "    print(\"Matrix Inverse:\")\n",
    "    print(inv_matrix.round(3))\n",
    "    \n",
    "    # Verify: A * A^(-1) = I\n",
    "    verification = test_matrix @ inv_matrix\n",
    "    print(\"Verification (A @ A^(-1)):\")\n",
    "    print(verification.round(10))  # Round to avoid floating point errors\n",
    "else:\n",
    "    print(\"Matrix is singular (not invertible)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize matrix operations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Sample matrices for visualization\n",
    "M1 = np.array([[3, 1], [1, 2]])\n",
    "M2 = np.array([[2, 0], [1, 1]])\n",
    "\n",
    "# Plot matrices as heatmaps\n",
    "im1 = axes[0, 0].imshow(M1, cmap='viridis', interpolation='nearest')\n",
    "axes[0, 0].set_title('Matrix M1')\n",
    "for i in range(M1.shape[0]):\n",
    "    for j in range(M1.shape[1]):\n",
    "        axes[0, 0].text(j, i, f'{M1[i, j]}', ha='center', va='center', color='white', fontsize=14)\n",
    "\n",
    "im2 = axes[0, 1].imshow(M2, cmap='viridis', interpolation='nearest')\n",
    "axes[0, 1].set_title('Matrix M2')\n",
    "for i in range(M2.shape[0]):\n",
    "    for j in range(M2.shape[1]):\n",
    "        axes[0, 1].text(j, i, f'{M2[i, j]}', ha='center', va='center', color='white', fontsize=14)\n",
    "\n",
    "# Matrix multiplication result\n",
    "M_product = M1 @ M2\n",
    "im3 = axes[1, 0].imshow(M_product, cmap='viridis', interpolation='nearest')\n",
    "axes[1, 0].set_title('M1 @ M2 (Matrix Product)')\n",
    "for i in range(M_product.shape[0]):\n",
    "    for j in range(M_product.shape[1]):\n",
    "        axes[1, 0].text(j, i, f'{M_product[i, j]}', ha='center', va='center', color='white', fontsize=14)\n",
    "\n",
    "# Element-wise multiplication\n",
    "M_elementwise = M1 * M2\n",
    "im4 = axes[1, 1].imshow(M_elementwise, cmap='viridis', interpolation='nearest')\n",
    "axes[1, 1].set_title('M1 * M2 (Element-wise)')\n",
    "for i in range(M_elementwise.shape[0]):\n",
    "    for j in range(M_elementwise.shape[1]):\n",
    "        axes[1, 1].text(j, i, f'{M_elementwise[i, j]}', ha='center', va='center', color='white', fontsize=14)\n",
    "\n",
    "# Remove ticks for cleaner look\n",
    "for ax in axes.flat:\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show the calculation step by step\n",
    "print(\"ðŸ” Matrix Multiplication Step by Step:\")\n",
    "print(\"=\" * 45)\n",
    "print(f\"M1 @ M2 calculation:\")\n",
    "print(f\"M1 = {M1.tolist()}\")\n",
    "print(f\"M2 = {M2.tolist()}\")\n",
    "print()\n",
    "\n",
    "for i in range(M1.shape[0]):\n",
    "    for j in range(M2.shape[1]):\n",
    "        row = M1[i, :]\n",
    "        col = M2[:, j]\n",
    "        result = np.dot(row, col)\n",
    "        print(f\"Position ({i},{j}): {row} Â· {col} = {result}\")\n",
    "\n",
    "print(f\"\\nResult: {M_product.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigenvalues and Eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eigenvalues and eigenvectors\n",
    "print(\"ðŸ”® Eigenvalues and Eigenvectors\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Create a symmetric matrix (real eigenvalues)\n",
    "A = np.array([[4, 2], [2, 3]])\n",
    "\n",
    "print(\"Matrix A:\")\n",
    "print(A)\n",
    "print()\n",
    "\n",
    "# Compute eigenvalues and eigenvectors\n",
    "eigenvalues, eigenvectors = np.linalg.eig(A)\n",
    "\n",
    "print(\"Eigenvalues:\")\n",
    "for i, val in enumerate(eigenvalues):\n",
    "    print(f\"Î»{i+1} = {val:.3f}\")\n",
    "print()\n",
    "\n",
    "print(\"Eigenvectors:\")\n",
    "for i, vec in enumerate(eigenvectors.T):\n",
    "    print(f\"v{i+1} = {vec}\")\n",
    "print()\n",
    "\n",
    "# Verify the eigenvalue equation: Av = Î»v\n",
    "print(\"Verification (Av = Î»v):\")\n",
    "for i in range(len(eigenvalues)):\n",
    "    Î» = eigenvalues[i]\n",
    "    v = eigenvectors[:, i]\n",
    "    \n",
    "    Av = A @ v\n",
    "    Î»v = Î» * v\n",
    "    \n",
    "    print(f\"\\nEigenvalue {i+1}: Î» = {Î»:.3f}\")\n",
    "    print(f\"Av = {Av.round(3)}\")\n",
    "    print(f\"Î»v = {Î»v.round(3)}\")\n",
    "    print(f\"Equal? {np.allclose(Av, Î»v)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "\n",
    "# Eigendecomposition\n",
    "# A = PDP^(-1) where P is eigenvectors, D is diagonal eigenvalues\n",
    "P = eigenvectors\n",
    "D = np.diag(eigenvalues)\n",
    "P_inv = np.linalg.inv(P)\n",
    "\n",
    "A_reconstructed = P @ D @ P_inv\n",
    "\n",
    "print(\"Eigendecomposition: A = PDP^(-1)\")\n",
    "print(f\"P (eigenvectors):\\n{P.round(3)}\")\n",
    "print(f\"\\nD (eigenvalues):\\n{D.round(3)}\")\n",
    "print(f\"\\nP^(-1):\\n{P_inv.round(3)}\")\n",
    "print(f\"\\nReconstructed A:\\n{A_reconstructed.round(3)}\")\n",
    "print(f\"\\nOriginal A:\\n{A}\")\n",
    "print(f\"Reconstruction accurate? {np.allclose(A, A_reconstructed)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize eigenvectors\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Plot 1: Eigenvectors of the matrix\n",
    "origin = [0, 0]\n",
    "\n",
    "# Plot eigenvectors\n",
    "for i, (val, vec) in enumerate(zip(eigenvalues, eigenvectors.T)):\n",
    "    color = ['red', 'blue'][i]\n",
    "    # Scale eigenvectors by eigenvalue for visualization\n",
    "    scaled_vec = vec * val * 0.5\n",
    "    axes[0].quiver(*origin, scaled_vec[0], scaled_vec[1], \n",
    "                   angles='xy', scale_units='xy', scale=1, \n",
    "                   color=color, width=0.008, \n",
    "                   label=f'v{i+1} (Î»={val:.2f})')\n",
    "    \n",
    "    # Add vector labels\n",
    "    axes[0].text(scaled_vec[0]*1.2, scaled_vec[1]*1.2, f'v{i+1}', \n",
    "                 fontsize=12, color=color)\n",
    "\n",
    "# Add unit circle for reference\n",
    "theta = np.linspace(0, 2*np.pi, 100)\n",
    "axes[0].plot(np.cos(theta), np.sin(theta), 'k--', alpha=0.3, label='Unit circle')\n",
    "\n",
    "axes[0].set_xlim(-3, 3)\n",
    "axes[0].set_ylim(-3, 3)\n",
    "axes[0].set_aspect('equal')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_title('Eigenvectors of Matrix A')\n",
    "axes[0].set_xlabel('x')\n",
    "axes[0].set_ylabel('y')\n",
    "axes[0].legend()\n",
    "\n",
    "# Plot 2: Transformation visualization\n",
    "# Create a set of points\n",
    "n_points = 20\n",
    "theta_points = np.linspace(0, 2*np.pi, n_points)\n",
    "unit_circle_points = np.array([np.cos(theta_points), np.sin(theta_points)])\n",
    "\n",
    "# Transform points by matrix A\n",
    "transformed_points = A @ unit_circle_points\n",
    "\n",
    "# Plot original and transformed points\n",
    "axes[1].plot(unit_circle_points[0], unit_circle_points[1], 'bo-', \n",
    "             alpha=0.6, label='Original (unit circle)', markersize=4)\n",
    "axes[1].plot(transformed_points[0], transformed_points[1], 'ro-', \n",
    "             alpha=0.6, label='Transformed by A', markersize=4)\n",
    "\n",
    "# Plot eigenvectors on transformed space\n",
    "for i, (val, vec) in enumerate(zip(eigenvalues, eigenvectors.T)):\n",
    "    color = ['darkred', 'darkblue'][i]\n",
    "    scaled_vec = vec * val\n",
    "    axes[1].quiver(*origin, scaled_vec[0], scaled_vec[1], \n",
    "                   angles='xy', scale_units='xy', scale=1, \n",
    "                   color=color, width=0.008)\n",
    "\n",
    "axes[1].set_xlim(-6, 6)\n",
    "axes[1].set_ylim(-6, 6)\n",
    "axes[1].set_aspect('equal')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_title('Linear Transformation by Matrix A')\n",
    "axes[1].set_xlabel('x')\n",
    "axes[1].set_ylabel('y')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸŽ¯ Eigenvalue/Eigenvector Interpretation:\")\n",
    "eigen_concepts = [\n",
    "    \"Eigenvectors: directions that don't change under transformation\",\n",
    "    \"Eigenvalues: how much the eigenvectors are scaled\",\n",
    "    \"Î» > 1: eigenvector is stretched\",\n",
    "    \"0 < Î» < 1: eigenvector is compressed\",\n",
    "    \"Î» < 0: eigenvector is flipped and scaled\",\n",
    "    \"Used in PCA, PageRank, stability analysis, quantum mechanics\"\n",
    "]\n",
    "\n",
    "for concept in eigen_concepts:\n",
    "    print(f\"â€¢ {concept}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Decomposition: SVD and PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Singular Value Decomposition (SVD)\n",
    "print(\"ðŸ”€ Singular Value Decomposition (SVD)\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Create a sample matrix\n",
    "np.random.seed(42)\n",
    "A = np.random.randn(4, 3)\n",
    "\n",
    "print(f\"Original matrix A ({A.shape[0]}Ã—{A.shape[1]}):\")\n",
    "print(A.round(3))\n",
    "print()\n",
    "\n",
    "# Perform SVD: A = UÎ£V^T\n",
    "U, s, Vt = np.linalg.svd(A, full_matrices=False)\n",
    "\n",
    "print(f\"U matrix ({U.shape[0]}Ã—{U.shape[1]}):\")\n",
    "print(U.round(3))\n",
    "print(f\"\\nSingular values s ({len(s)} values):\")\n",
    "print(s.round(3))\n",
    "print(f\"\\nV^T matrix ({Vt.shape[0]}Ã—{Vt.shape[1]}):\")\n",
    "print(Vt.round(3))\n",
    "print()\n",
    "\n",
    "# Reconstruct the matrix\n",
    "S = np.diag(s)\n",
    "A_reconstructed = U @ S @ Vt\n",
    "\n",
    "print(\"Reconstructed A:\")\n",
    "print(A_reconstructed.round(3))\n",
    "print(f\"Reconstruction error: {np.linalg.norm(A - A_reconstructed):.2e}\")\n",
    "print()\n",
    "\n",
    "# Low-rank approximation\n",
    "print(\"Low-rank approximations:\")\n",
    "for k in range(1, min(A.shape) + 1):\n",
    "    # Use only first k components\n",
    "    A_k = U[:, :k] @ np.diag(s[:k]) @ Vt[:k, :]\n",
    "    error = np.linalg.norm(A - A_k)\n",
    "    variance_explained = np.sum(s[:k]**2) / np.sum(s**2) * 100\n",
    "    print(f\"  Rank-{k}: Error = {error:.3f}, Variance explained = {variance_explained:.1f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*45)\n",
    "\n",
    "# Principal Component Analysis (PCA)\n",
    "print(\"ðŸ“Š Principal Component Analysis (PCA)\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Generate sample 2D data\n",
    "np.random.seed(42)\n",
    "n_samples = 100\n",
    "\n",
    "# Create correlated data\n",
    "X = np.random.randn(n_samples, 2)\n",
    "X[:, 1] = X[:, 0] + 0.5 * np.random.randn(n_samples)  # Add correlation\n",
    "\n",
    "# Add some rotation\n",
    "angle = np.pi / 6  # 30 degrees\n",
    "rotation_matrix = np.array([[np.cos(angle), -np.sin(angle)],\n",
    "                           [np.sin(angle), np.cos(angle)]])\n",
    "X = X @ rotation_matrix.T\n",
    "\n",
    "# Center the data\n",
    "X_centered = X - np.mean(X, axis=0)\n",
    "\n",
    "# Perform PCA manually using SVD\n",
    "U, s, Vt = np.linalg.svd(X_centered, full_matrices=False)\n",
    "principal_components = Vt.T  # Each column is a principal component\n",
    "\n",
    "# Explained variance\n",
    "explained_variance = s**2 / (n_samples - 1)\n",
    "explained_variance_ratio = explained_variance / np.sum(explained_variance)\n",
    "\n",
    "print(f\"Data shape: {X.shape}\")\n",
    "print(f\"Data mean: {np.mean(X, axis=0).round(3)}\")\n",
    "print(f\"Data covariance matrix:\")\n",
    "print(np.cov(X.T).round(3))\n",
    "print()\n",
    "\n",
    "print(\"Principal Components:\")\n",
    "for i in range(len(explained_variance)):\n",
    "    print(f\"PC{i+1}: {principal_components[:, i].round(3)} (explains {explained_variance_ratio[i]*100:.1f}% variance)\")\n",
    "print()\n",
    "\n",
    "# Transform data to PC space\n",
    "X_pca = X_centered @ principal_components\n",
    "\n",
    "print(f\"Transformed data shape: {X_pca.shape}\")\n",
    "print(f\"PC1 variance: {np.var(X_pca[:, 0]):.3f}\")\n",
    "print(f\"PC2 variance: {np.var(X_pca[:, 1]):.3f}\")\n",
    "print(f\"Correlation between PCs: {np.corrcoef(X_pca.T)[0, 1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize PCA\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Plot 1: Original data with principal components\n",
    "axes[0].scatter(X[:, 0], X[:, 1], alpha=0.6, c='blue')\n",
    "\n",
    "# Plot principal components\n",
    "center = np.mean(X, axis=0)\n",
    "for i, (pc, var_ratio) in enumerate(zip(principal_components.T, explained_variance_ratio)):\n",
    "    color = ['red', 'green'][i]\n",
    "    # Scale by sqrt of eigenvalue for visualization\n",
    "    pc_scaled = pc * np.sqrt(explained_variance[i]) * 2\n",
    "    axes[0].arrow(center[0], center[1], pc_scaled[0], pc_scaled[1],\n",
    "                  head_width=0.1, head_length=0.1, fc=color, ec=color, linewidth=2)\n",
    "    axes[0].text(center[0] + pc_scaled[0]*1.2, center[1] + pc_scaled[1]*1.2, \n",
    "                 f'PC{i+1}\\n({var_ratio*100:.1f}%)', fontsize=10, color=color, ha='center')\n",
    "\n",
    "axes[0].set_aspect('equal')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_title('Original Data with Principal Components')\n",
    "axes[0].set_xlabel('X1')\n",
    "axes[0].set_ylabel('X2')\n",
    "\n",
    "# Plot 2: Data in PC space\n",
    "axes[1].scatter(X_pca[:, 0], X_pca[:, 1], alpha=0.6, c='purple')\n",
    "axes[1].axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "axes[1].axvline(x=0, color='k', linestyle='-', alpha=0.3)\n",
    "axes[1].set_aspect('equal')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_title('Data in Principal Component Space')\n",
    "axes[1].set_xlabel('PC1')\n",
    "axes[1].set_ylabel('PC2')\n",
    "\n",
    "# Plot 3: Explained variance\n",
    "pcs = [f'PC{i+1}' for i in range(len(explained_variance_ratio))]\n",
    "axes[2].bar(pcs, explained_variance_ratio * 100, color=['red', 'green'])\n",
    "axes[2].set_title('Explained Variance by Component')\n",
    "axes[2].set_ylabel('Explained Variance (%)')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "# Add cumulative variance line\n",
    "cumulative_var = np.cumsum(explained_variance_ratio) * 100\n",
    "ax2_twin = axes[2].twinx()\n",
    "ax2_twin.plot(pcs, cumulative_var, 'bo-', color='orange', linewidth=2, markersize=8)\n",
    "ax2_twin.set_ylabel('Cumulative Explained Variance (%)', color='orange')\n",
    "ax2_twin.tick_params(axis='y', labelcolor='orange')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸŽ¯ PCA Key Insights:\")\n",
    "pca_insights = [\n",
    "    \"PCA finds directions of maximum variance in data\",\n",
    "    \"Principal components are orthogonal (uncorrelated)\",\n",
    "    \"First PC captures most variance, second PC captures second most, etc.\",\n",
    "    \"Used for dimensionality reduction and data visualization\",\n",
    "    \"Helps identify most important features in high-dimensional data\",\n",
    "    \"Foundation for many ML algorithms and data compression\"\n",
    "]\n",
    "\n",
    "for insight in pca_insights:\n",
    "    print(f\"â€¢ {insight}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications in Machine Learning and NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Application 1: Document similarity using cosine similarity\n",
    "print(\"ðŸ“š Application 1: Document Similarity (NLP)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Simulate document-term matrix (documents as rows, terms as columns)\n",
    "documents = [\n",
    "    \"machine learning algorithms\",\n",
    "    \"deep learning neural networks\", \n",
    "    \"natural language processing\",\n",
    "    \"computer vision image recognition\",\n",
    "    \"data science analytics\"\n",
    "]\n",
    "\n",
    "# Simple bag-of-words representation (in practice, use TF-IDF)\n",
    "vocabulary = ['machine', 'learning', 'algorithms', 'deep', 'neural', 'networks', \n",
    "              'natural', 'language', 'processing', 'computer', 'vision', 'image', \n",
    "              'recognition', 'data', 'science', 'analytics']\n",
    "\n",
    "# Create document-term matrix\n",
    "doc_term_matrix = np.zeros((len(documents), len(vocabulary)))\n",
    "\n",
    "for i, doc in enumerate(documents):\n",
    "    words = doc.split()\n",
    "    for word in words:\n",
    "        if word in vocabulary:\n",
    "            j = vocabulary.index(word)\n",
    "            doc_term_matrix[i, j] = 1  # Binary occurrence\n",
    "\n",
    "print(\"Document-Term Matrix:\")\n",
    "print(f\"Shape: {doc_term_matrix.shape}\")\n",
    "print(doc_term_matrix.astype(int))\n",
    "print()\n",
    "\n",
    "# Calculate cosine similarity between documents\n",
    "def cosine_similarity(v1, v2):\n",
    "    \"\"\"Calculate cosine similarity between two vectors.\"\"\"\n",
    "    dot_product = np.dot(v1, v2)\n",
    "    norm_product = np.linalg.norm(v1) * np.linalg.norm(v2)\n",
    "    if norm_product == 0:\n",
    "        return 0\n",
    "    return dot_product / norm_product\n",
    "\n",
    "# Create similarity matrix\n",
    "n_docs = len(documents)\n",
    "similarity_matrix = np.zeros((n_docs, n_docs))\n",
    "\n",
    "for i in range(n_docs):\n",
    "    for j in range(n_docs):\n",
    "        similarity_matrix[i, j] = cosine_similarity(doc_term_matrix[i], doc_term_matrix[j])\n",
    "\n",
    "print(\"Document Similarity Matrix (Cosine Similarity):\")\n",
    "print(similarity_matrix.round(3))\n",
    "print()\n",
    "\n",
    "# Find most similar documents\n",
    "print(\"Document pairs and their similarities:\")\n",
    "for i in range(n_docs):\n",
    "    for j in range(i+1, n_docs):\n",
    "        sim = similarity_matrix[i, j]\n",
    "        print(f\"Doc {i+1} & Doc {j+1}: {sim:.3f}\")\n",
    "        print(f\"  '{documents[i]}' vs '{documents[j]}'\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Application 2: Linear regression using linear algebra\n",
    "print(\"ðŸ“ˆ Application 2: Linear Regression\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Generate synthetic data\n",
    "np.random.seed(42)\n",
    "n_samples = 100\n",
    "X = np.random.randn(n_samples, 2)  # 2 features\n",
    "true_coefficients = np.array([3, -2])\n",
    "true_intercept = 1\n",
    "noise = np.random.randn(n_samples) * 0.5\n",
    "\n",
    "# y = X @ coefficients + intercept + noise\n",
    "y = X @ true_coefficients + true_intercept + noise\n",
    "\n",
    "print(f\"Data shape: X = {X.shape}, y = {y.shape}\")\n",
    "print(f\"True coefficients: {true_coefficients}\")\n",
    "print(f\"True intercept: {true_intercept}\")\n",
    "print()\n",
    "\n",
    "# Add intercept term to X (bias column)\n",
    "X_with_intercept = np.column_stack([np.ones(n_samples), X])\n",
    "\n",
    "# Solve using normal equation: Î¸ = (X^T X)^(-1) X^T y\n",
    "XtX = X_with_intercept.T @ X_with_intercept\n",
    "Xty = X_with_intercept.T @ y\n",
    "theta = np.linalg.solve(XtX, Xty)  # More stable than using inverse\n",
    "\n",
    "estimated_intercept = theta[0]\n",
    "estimated_coefficients = theta[1:]\n",
    "\n",
    "print(\"Linear Regression Results:\")\n",
    "print(f\"Estimated intercept: {estimated_intercept:.3f} (true: {true_intercept})\")\n",
    "print(f\"Estimated coefficients: {estimated_coefficients.round(3)} (true: {true_coefficients})\")\n",
    "print()\n",
    "\n",
    "# Calculate predictions and error\n",
    "y_pred = X_with_intercept @ theta\n",
    "mse = np.mean((y - y_pred)**2)\n",
    "r_squared = 1 - np.sum((y - y_pred)**2) / np.sum((y - np.mean(y))**2)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "print(f\"R-squared: {r_squared:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Application 3: Dimensionality reduction with PCA\n",
    "print(\"ðŸŽ¯ Application 3: Dimensionality Reduction\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Generate high-dimensional data with intrinsic lower dimensionality\n",
    "np.random.seed(42)\n",
    "n_samples = 200\n",
    "n_features = 10\n",
    "\n",
    "# Create data that lies mostly in a 2D subspace\n",
    "latent_dim = 2\n",
    "latent_data = np.random.randn(n_samples, latent_dim)\n",
    "\n",
    "# Random projection to higher dimensional space\n",
    "projection_matrix = np.random.randn(latent_dim, n_features)\n",
    "X_high_dim = latent_data @ projection_matrix\n",
    "\n",
    "# Add some noise\n",
    "X_high_dim += np.random.randn(n_samples, n_features) * 0.1\n",
    "\n",
    "print(f\"High-dimensional data shape: {X_high_dim.shape}\")\n",
    "\n",
    "# Apply PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA()\n",
    "X_pca = pca.fit_transform(X_high_dim)\n",
    "\n",
    "# Analyze explained variance\n",
    "explained_var_ratio = pca.explained_variance_ratio_\n",
    "cumulative_var = np.cumsum(explained_var_ratio)\n",
    "\n",
    "print(\"\\nExplained variance by component:\")\n",
    "for i, var_ratio in enumerate(explained_var_ratio[:5]):\n",
    "    print(f\"PC{i+1}: {var_ratio:.4f} ({var_ratio*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\nCumulative variance explained by first 2 components: {cumulative_var[1]:.4f} ({cumulative_var[1]*100:.2f}%)\")\n",
    "print(f\"Cumulative variance explained by first 5 components: {cumulative_var[4]:.4f} ({cumulative_var[4]*100:.2f}%)\")\n",
    "\n",
    "# Determine number of components for 95% variance\n",
    "n_components_95 = np.argmax(cumulative_var >= 0.95) + 1\n",
    "print(f\"\\nComponents needed for 95% variance: {n_components_95}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize applications\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Plot 1: Document similarity heatmap\n",
    "im1 = axes[0, 0].imshow(similarity_matrix, cmap='viridis', interpolation='nearest')\n",
    "axes[0, 0].set_title('Document Similarity Matrix')\n",
    "axes[0, 0].set_xlabel('Document')\n",
    "axes[0, 0].set_ylabel('Document')\n",
    "axes[0, 0].set_xticks(range(n_docs))\n",
    "axes[0, 0].set_yticks(range(n_docs))\n",
    "axes[0, 0].set_xticklabels([f'Doc{i+1}' for i in range(n_docs)])\n",
    "axes[0, 0].set_yticklabels([f'Doc{i+1}' for i in range(n_docs)])\n",
    "\n",
    "# Add similarity values to heatmap\n",
    "for i in range(n_docs):\n",
    "    for j in range(n_docs):\n",
    "        axes[0, 0].text(j, i, f'{similarity_matrix[i, j]:.2f}', \n",
    "                        ha='center', va='center', color='white')\n",
    "\n",
    "plt.colorbar(im1, ax=axes[0, 0])\n",
    "\n",
    "# Plot 2: Linear regression results\n",
    "axes[0, 1].scatter(y, y_pred, alpha=0.6)\n",
    "min_val, max_val = min(y.min(), y_pred.min()), max(y.max(), y_pred.max())\n",
    "axes[0, 1].plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2)\n",
    "axes[0, 1].set_xlabel('True y')\n",
    "axes[0, 1].set_ylabel('Predicted y')\n",
    "axes[0, 1].set_title(f'Linear Regression Results\\nRÂ² = {r_squared:.3f}')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: PCA explained variance\n",
    "n_components_show = 8\n",
    "axes[1, 0].bar(range(1, n_components_show + 1), \n",
    "               explained_var_ratio[:n_components_show] * 100, \n",
    "               alpha=0.7, color='skyblue')\n",
    "axes[1, 0].plot(range(1, n_components_show + 1), \n",
    "                cumulative_var[:n_components_show] * 100, \n",
    "                'ro-', linewidth=2, markersize=6)\n",
    "axes[1, 0].set_xlabel('Principal Component')\n",
    "axes[1, 0].set_ylabel('Explained Variance (%)')\n",
    "axes[1, 0].set_title('PCA Explained Variance')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "axes[1, 0].axhline(y=95, color='red', linestyle='--', alpha=0.7, label='95% threshold')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# Plot 4: First two principal components\n",
    "axes[1, 1].scatter(X_pca[:, 0], X_pca[:, 1], alpha=0.6, c=latent_data[:, 0], cmap='viridis')\n",
    "axes[1, 1].set_xlabel('First Principal Component')\n",
    "axes[1, 1].set_ylabel('Second Principal Component')\n",
    "axes[1, 1].set_title('Data in PC Space (colored by latent feature)')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸš€ Linear Algebra in ML/NLP Summary:\")\n",
    "applications = [\n",
    "    \"Document similarity: Cosine similarity using dot products\",\n",
    "    \"Linear regression: Matrix operations for parameter estimation\",\n",
    "    \"PCA: Eigendecomposition for dimensionality reduction\",\n",
    "    \"Neural networks: Matrix multiplications in forward/backward pass\",\n",
    "    \"Word embeddings: Vector representations of words\",\n",
    "    \"SVD: Matrix factorization for recommendation systems\",\n",
    "    \"Optimization: Gradient descent uses vector operations\",\n",
    "    \"Computer vision: Convolutions as matrix operations\"\n",
    "]\n",
    "\n",
    "for app in applications:\n",
    "    print(f\"â€¢ {app}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### Essential Linear Algebra Concepts:\n",
    "\n",
    "1. **Vectors**:\n",
    "   - Represent data points, features, parameters\n",
    "   - Dot product measures similarity/projection\n",
    "   - Norms measure magnitude/distance\n",
    "   - Foundation for all ML algorithms\n",
    "\n",
    "2. **Matrices**:\n",
    "   - Represent datasets, transformations, systems\n",
    "   - Matrix multiplication combines transformations\n",
    "   - Transpose, inverse, determinant have geometric meaning\n",
    "   - Used in data representation and model parameters\n",
    "\n",
    "3. **Eigendecomposition**:\n",
    "   - Finds principal directions of transformation\n",
    "   - Used in PCA, stability analysis, PageRank\n",
    "   - Eigenvalues show importance of directions\n",
    "   - Eigenvectors are the principal directions\n",
    "\n",
    "4. **SVD (Singular Value Decomposition)**:\n",
    "   - Most general matrix decomposition\n",
    "   - Works for any matrix (not just square)\n",
    "   - Foundation of PCA, recommender systems\n",
    "   - Enables low-rank approximations\n",
    "\n",
    "5. **PCA (Principal Component Analysis)**:\n",
    "   - Finds directions of maximum variance\n",
    "   - Dimensionality reduction technique\n",
    "   - Data compression and visualization\n",
    "   - Removes correlation between features\n",
    "\n",
    "### Applications in Data Science:\n",
    "\n",
    "**Machine Learning:**\n",
    "- Linear/logistic regression: Matrix operations\n",
    "- Neural networks: Matrix multiplications\n",
    "- SVM: Inner products in feature space\n",
    "- Clustering: Distance calculations\n",
    "\n",
    "**NLP:**\n",
    "- Word embeddings: Vector representations\n",
    "- Document similarity: Cosine similarity\n",
    "- Topic modeling: Matrix factorization\n",
    "- Transformers: Attention via matrix operations\n",
    "\n",
    "**Computer Vision:**\n",
    "- Image as matrices\n",
    "- Convolutions: Matrix operations\n",
    "- PCA for face recognition\n",
    "- Image transformations\n",
    "\n",
    "### Essential NumPy Operations:\n",
    "\n",
    "```python\n",
    "# Vector operations\n",
    "np.dot(v1, v2)          # Dot product\n",
    "np.linalg.norm(v)       # Vector norm\n",
    "v / np.linalg.norm(v)   # Unit vector\n",
    "\n",
    "# Matrix operations\n",
    "A @ B                   # Matrix multiplication\n",
    "A.T                     # Transpose\n",
    "np.linalg.inv(A)        # Inverse\n",
    "np.linalg.det(A)        # Determinant\n",
    "\n",
    "# Decompositions\n",
    "eigenvals, eigenvecs = np.linalg.eig(A)    # Eigendecomposition\n",
    "U, s, Vt = np.linalg.svd(A)               # SVD\n",
    "```\n",
    "\n",
    "### Why Linear Algebra Matters:\n",
    "\n",
    "1. **Efficiency**: Vectorized operations are much faster\n",
    "2. **Scalability**: Handle large datasets efficiently\n",
    "3. **Generality**: Same operations work in any dimension\n",
    "4. **Optimization**: Gradient-based methods use linear algebra\n",
    "5. **Understanding**: Provides geometric intuition for algorithms\n",
    "\n",
    "## Practice Exercises\n",
    "\n",
    "1. **Implement linear regression** from scratch using only NumPy\n",
    "2. **Build a simple neural network** with matrix operations\n",
    "3. **Create a document recommender** using cosine similarity\n",
    "4. **Perform image compression** using SVD\n",
    "5. **Implement PCA** for data visualization\n",
    "6. **Solve a system of equations** using different methods\n",
    "7. **Create a PageRank algorithm** using eigenvalues\n",
    "8. **Build a collaborative filtering system** with matrix factorization\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Master linear algebra to:\n",
    "- **Understand ML algorithms** at a deeper level\n",
    "- **Debug and optimize** model performance\n",
    "- **Implement custom algorithms** efficiently\n",
    "- **Work with high-dimensional data** confidently\n",
    "- **Contribute to research** in ML/AI\n",
    "\n",
    "Linear algebra is the language of machine learning â€“ master it to unlock the full potential of data science!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}