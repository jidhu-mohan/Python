{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Testing and Debugging\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this notebook, you will understand:\n",
    "- Unit testing with unittest and pytest\n",
    "- Test-driven development (TDD)\n",
    "- Debugging techniques and tools\n",
    "- Code coverage and quality metrics\n",
    "- Integration and functional testing\n",
    "- Mock objects and fixtures\n",
    "\n",
    "## Why Testing and Debugging Matter for ML/NLP\n",
    "- Ensures data pipeline reliability\n",
    "- Validates model performance\n",
    "- Catches preprocessing errors\n",
    "- Maintains code quality in large projects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example function to test\n",
    "def calculate_accuracy(true_positives, true_negatives, total_samples):\n",
    "    \"\"\"\n",
    "    Calculate accuracy for a classification model.\n",
    "    \n",
    "    Args:\n",
    "        true_positives (int): Number of correctly predicted positive cases\n",
    "        true_negatives (int): Number of correctly predicted negative cases\n",
    "        total_samples (int): Total number of samples\n",
    "    \n",
    "    Returns:\n",
    "        float: Accuracy score between 0 and 1\n",
    "    \"\"\"\n",
    "    if total_samples <= 0:\n",
    "        raise ValueError(\"Total samples must be positive\")\n",
    "    \n",
    "    return (true_positives + true_negatives) / total_samples\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Simple text preprocessing for NLP.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        raise TypeError(\"Input must be a string\")\n",
    "    \n",
    "    # Convert to lowercase and remove extra spaces\n",
    "    return ' '.join(text.lower().split())\n",
    "\n",
    "# Test the functions\n",
    "print(f\"Accuracy: {calculate_accuracy(85, 90, 200)}\")\n",
    "print(f\"Preprocessed: '{preprocess_text('  Hello   WORLD  ')}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Unit Testing with unittest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "import sys\n",
    "from io import StringIO\n",
    "\n",
    "class TestMLFunctions(unittest.TestCase):\n",
    "    \"\"\"\n",
    "    Unit tests for ML utility functions.\n",
    "    \"\"\"\n",
    "    \n",
    "    def test_calculate_accuracy_valid_input(self):\n",
    "        \"\"\"Test accuracy calculation with valid inputs.\"\"\"\n",
    "        result = calculate_accuracy(85, 90, 200)\n",
    "        self.assertAlmostEqual(result, 0.875, places=3)\n",
    "    \n",
    "    def test_calculate_accuracy_perfect_score(self):\n",
    "        \"\"\"Test perfect accuracy score.\"\"\"\n",
    "        result = calculate_accuracy(50, 50, 100)\n",
    "        self.assertEqual(result, 1.0)\n",
    "    \n",
    "    def test_calculate_accuracy_zero_correct(self):\n",
    "        \"\"\"Test accuracy with no correct predictions.\"\"\"\n",
    "        result = calculate_accuracy(0, 0, 100)\n",
    "        self.assertEqual(result, 0.0)\n",
    "    \n",
    "    def test_calculate_accuracy_invalid_total(self):\n",
    "        \"\"\"Test accuracy calculation with invalid total samples.\"\"\"\n",
    "        with self.assertRaises(ValueError):\n",
    "            calculate_accuracy(10, 10, 0)\n",
    "        \n",
    "        with self.assertRaises(ValueError):\n",
    "            calculate_accuracy(10, 10, -5)\n",
    "    \n",
    "    def test_preprocess_text_normal_case(self):\n",
    "        \"\"\"Test text preprocessing with normal input.\"\"\"\n",
    "        result = preprocess_text(\"  Hello   WORLD  \")\n",
    "        self.assertEqual(result, \"hello world\")\n",
    "    \n",
    "    def test_preprocess_text_empty_string(self):\n",
    "        \"\"\"Test text preprocessing with empty string.\"\"\"\n",
    "        result = preprocess_text(\"\")\n",
    "        self.assertEqual(result, \"\")\n",
    "    \n",
    "    def test_preprocess_text_invalid_type(self):\n",
    "        \"\"\"Test text preprocessing with invalid input type.\"\"\"\n",
    "        with self.assertRaises(TypeError):\n",
    "            preprocess_text(123)\n",
    "        \n",
    "        with self.assertRaises(TypeError):\n",
    "            preprocess_text(None)\n",
    "\n",
    "# Run the tests\n",
    "if __name__ == '__main__':\n",
    "    # Capture test output\n",
    "    test_output = StringIO()\n",
    "    runner = unittest.TextTestRunner(stream=test_output, verbosity=2)\n",
    "    suite = unittest.TestLoader().loadTestsFromTestCase(TestMLFunctions)\n",
    "    result = runner.run(suite)\n",
    "    \n",
    "    print(test_output.getvalue())\n",
    "    print(f\"\\nTests run: {result.testsRun}\")\n",
    "    print(f\"Failures: {len(result.failures)}\")\n",
    "    print(f\"Errors: {len(result.errors)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test-Driven Development (TDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TDD Example: Implementing a confusion matrix calculator\n",
    "\n",
    "# Step 1: Write the test first\n",
    "class TestConfusionMatrix(unittest.TestCase):\n",
    "    def test_binary_confusion_matrix(self):\n",
    "        \"\"\"Test binary classification confusion matrix.\"\"\"\n",
    "        y_true = [1, 1, 0, 0, 1, 0, 1, 0]\n",
    "        y_pred = [1, 0, 0, 0, 1, 1, 1, 0]\n",
    "        \n",
    "        expected = {\n",
    "            'tp': 3,  # True Positives\n",
    "            'tn': 3,  # True Negatives \n",
    "            'fp': 1,  # False Positives\n",
    "            'fn': 1   # False Negatives\n",
    "        }\n",
    "        \n",
    "        result = confusion_matrix_binary(y_true, y_pred)\n",
    "        self.assertEqual(result, expected)\n",
    "    \n",
    "    def test_confusion_matrix_metrics(self):\n",
    "        \"\"\"Test precision, recall, F1 calculation from confusion matrix.\"\"\"\n",
    "        cm = {'tp': 3, 'tn': 3, 'fp': 1, 'fn': 1}\n",
    "        \n",
    "        metrics = calculate_metrics_from_cm(cm)\n",
    "        \n",
    "        self.assertAlmostEqual(metrics['precision'], 0.75, places=2)\n",
    "        self.assertAlmostEqual(metrics['recall'], 0.75, places=2)\n",
    "        self.assertAlmostEqual(metrics['f1'], 0.75, places=2)\n",
    "\n",
    "# Step 2: Implement the function to make tests pass\n",
    "def confusion_matrix_binary(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate confusion matrix for binary classification.\n",
    "    \n",
    "    Args:\n",
    "        y_true (list): True labels\n",
    "        y_pred (list): Predicted labels\n",
    "    \n",
    "    Returns:\n",
    "        dict: Confusion matrix components\n",
    "    \"\"\"\n",
    "    if len(y_true) != len(y_pred):\n",
    "        raise ValueError(\"y_true and y_pred must have same length\")\n",
    "    \n",
    "    tp = sum(1 for true, pred in zip(y_true, y_pred) if true == 1 and pred == 1)\n",
    "    tn = sum(1 for true, pred in zip(y_true, y_pred) if true == 0 and pred == 0)\n",
    "    fp = sum(1 for true, pred in zip(y_true, y_pred) if true == 0 and pred == 1)\n",
    "    fn = sum(1 for true, pred in zip(y_true, y_pred) if true == 1 and pred == 0)\n",
    "    \n",
    "    return {'tp': tp, 'tn': tn, 'fp': fp, 'fn': fn}\n",
    "\n",
    "def calculate_metrics_from_cm(cm):\n",
    "    \"\"\"\n",
    "    Calculate precision, recall, and F1 score from confusion matrix.\n",
    "    \n",
    "    Args:\n",
    "        cm (dict): Confusion matrix with tp, tn, fp, fn keys\n",
    "    \n",
    "    Returns:\n",
    "        dict: Precision, recall, and F1 scores\n",
    "    \"\"\"\n",
    "    tp, tn, fp, fn = cm['tp'], cm['tn'], cm['fp'], cm['fn']\n",
    "    \n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "\n",
    "# Step 3: Run the tests\n",
    "test_output = StringIO()\n",
    "runner = unittest.TextTestRunner(stream=test_output, verbosity=2)\n",
    "suite = unittest.TestLoader().loadTestsFromTestCase(TestConfusionMatrix)\n",
    "result = runner.run(suite)\n",
    "\n",
    "print(test_output.getvalue())\n",
    "print(f\"\\nTDD Tests - Run: {result.testsRun}, Failures: {len(result.failures)}, Errors: {len(result.errors)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Advanced Testing with Fixtures and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import os\n",
    "import json\n",
    "\n",
    "class TestDataProcessor(unittest.TestCase):\n",
    "    \"\"\"\n",
    "    Test class demonstrating fixtures and setup/teardown.\n",
    "    \"\"\"\n",
    "    \n",
    "    def setUp(self):\n",
    "        \"\"\"Set up test fixtures before each test method.\"\"\"\n",
    "        self.temp_dir = tempfile.mkdtemp()\n",
    "        \n",
    "        # Create sample data files\n",
    "        self.sample_data = {\n",
    "            'texts': ['hello world', 'machine learning', 'natural language'],\n",
    "            'labels': [1, 0, 1]\n",
    "        }\n",
    "        \n",
    "        self.data_file = os.path.join(self.temp_dir, 'sample_data.json')\n",
    "        with open(self.data_file, 'w') as f:\n",
    "            json.dump(self.sample_data, f)\n",
    "    \n",
    "    def tearDown(self):\n",
    "        \"\"\"Clean up after each test method.\"\"\"\n",
    "        import shutil\n",
    "        shutil.rmtree(self.temp_dir)\n",
    "    \n",
    "    def test_load_data(self):\n",
    "        \"\"\"Test data loading functionality.\"\"\"\n",
    "        data = load_json_data(self.data_file)\n",
    "        self.assertEqual(data, self.sample_data)\n",
    "    \n",
    "    def test_data_validation(self):\n",
    "        \"\"\"Test data validation.\"\"\"\n",
    "        is_valid = validate_text_data(self.sample_data)\n",
    "        self.assertTrue(is_valid)\n",
    "        \n",
    "        # Test invalid data\n",
    "        invalid_data = {'texts': ['hello'], 'labels': [1, 0]}  # Mismatched lengths\n",
    "        is_valid = validate_text_data(invalid_data)\n",
    "        self.assertFalse(is_valid)\n",
    "\n",
    "# Functions to test\n",
    "def load_json_data(filepath):\n",
    "    \"\"\"Load data from JSON file.\"\"\"\n",
    "    with open(filepath, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def validate_text_data(data):\n",
    "    \"\"\"Validate text data format.\"\"\"\n",
    "    if 'texts' not in data or 'labels' not in data:\n",
    "        return False\n",
    "    \n",
    "    if len(data['texts']) != len(data['labels']):\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Run the tests\n",
    "test_output = StringIO()\n",
    "runner = unittest.TextTestRunner(stream=test_output, verbosity=2)\n",
    "suite = unittest.TestLoader().loadTestsFromTestCase(TestDataProcessor)\n",
    "result = runner.run(suite)\n",
    "\n",
    "print(test_output.getvalue())\n",
    "print(f\"\\nFixture Tests - Run: {result.testsRun}, Failures: {len(result.failures)}, Errors: {len(result.errors)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Mock Objects and Dependency Injection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unittest.mock import Mock, patch, MagicMock\n",
    "import requests\n",
    "\n",
    "# Example API client to test\n",
    "class MLModelAPI:\n",
    "    \"\"\"Simple ML model API client.\"\"\"\n",
    "    \n",
    "    def __init__(self, base_url):\n",
    "        self.base_url = base_url\n",
    "    \n",
    "    def predict(self, text):\n",
    "        \"\"\"Get prediction from API.\"\"\"\n",
    "        response = requests.post(\n",
    "            f\"{self.base_url}/predict\",\n",
    "            json={'text': text}\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    \n",
    "    def get_model_info(self):\n",
    "        \"\"\"Get model information.\"\"\"\n",
    "        response = requests.get(f\"{self.base_url}/info\")\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "\n",
    "class TestMLModelAPI(unittest.TestCase):\n",
    "    \"\"\"Test ML API client using mocks.\"\"\"\n",
    "    \n",
    "    def setUp(self):\n",
    "        self.api = MLModelAPI('http://test-api.com')\n",
    "    \n",
    "    @patch('requests.post')\n",
    "    def test_predict_success(self, mock_post):\n",
    "        \"\"\"Test successful prediction.\"\"\"\n",
    "        # Mock the response\n",
    "        mock_response = Mock()\n",
    "        mock_response.json.return_value = {\n",
    "            'prediction': 'positive',\n",
    "            'confidence': 0.95\n",
    "        }\n",
    "        mock_response.raise_for_status.return_value = None\n",
    "        mock_post.return_value = mock_response\n",
    "        \n",
    "        # Test the method\n",
    "        result = self.api.predict('This is great!')\n",
    "        \n",
    "        # Assertions\n",
    "        self.assertEqual(result['prediction'], 'positive')\n",
    "        self.assertEqual(result['confidence'], 0.95)\n",
    "        \n",
    "        # Verify the mock was called correctly\n",
    "        mock_post.assert_called_once_with(\n",
    "            'http://test-api.com/predict',\n",
    "            json={'text': 'This is great!'}\n",
    "        )\n",
    "    \n",
    "    @patch('requests.post')\n",
    "    def test_predict_http_error(self, mock_post):\n",
    "        \"\"\"Test API error handling.\"\"\"\n",
    "        # Mock HTTP error\n",
    "        mock_response = Mock()\n",
    "        mock_response.raise_for_status.side_effect = requests.HTTPError('500 Server Error')\n",
    "        mock_post.return_value = mock_response\n",
    "        \n",
    "        # Test that exception is raised\n",
    "        with self.assertRaises(requests.HTTPError):\n",
    "            self.api.predict('test text')\n",
    "    \n",
    "    @patch('requests.get')\n",
    "    def test_get_model_info(self, mock_get):\n",
    "        \"\"\"Test model info retrieval.\"\"\"\n",
    "        # Mock successful response\n",
    "        mock_response = Mock()\n",
    "        mock_response.json.return_value = {\n",
    "            'model_name': 'bert-sentiment',\n",
    "            'version': '1.0.0',\n",
    "            'accuracy': 0.92\n",
    "        }\n",
    "        mock_response.raise_for_status.return_value = None\n",
    "        mock_get.return_value = mock_response\n",
    "        \n",
    "        result = self.api.get_model_info()\n",
    "        \n",
    "        self.assertEqual(result['model_name'], 'bert-sentiment')\n",
    "        self.assertEqual(result['accuracy'], 0.92)\n",
    "        mock_get.assert_called_once_with('http://test-api.com/info')\n",
    "\n",
    "# Run mock tests\n",
    "test_output = StringIO()\n",
    "runner = unittest.TextTestRunner(stream=test_output, verbosity=2)\n",
    "suite = unittest.TestLoader().loadTestsFromTestCase(TestMLModelAPI)\n",
    "result = runner.run(suite)\n",
    "\n",
    "print(test_output.getvalue())\n",
    "print(f\"\\nMock Tests - Run: {result.testsRun}, Failures: {len(result.failures)}, Errors: {len(result.errors)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Debugging Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import traceback\n",
    "from functools import wraps\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def debug_decorator(func):\n",
    "    \"\"\"Decorator to add debugging information to functions.\"\"\"\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        logger.debug(f\"Calling {func.__name__} with args={args}, kwargs={kwargs}\")\n",
    "        try:\n",
    "            result = func(*args, **kwargs)\n",
    "            logger.debug(f\"{func.__name__} returned: {result}\")\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in {func.__name__}: {str(e)}\")\n",
    "            logger.error(f\"Traceback: {traceback.format_exc()}\")\n",
    "            raise\n",
    "    return wrapper\n",
    "\n",
    "@debug_decorator\n",
    "def process_ml_data(data, model_type='linear'):\n",
    "    \"\"\"Process ML data with debugging.\"\"\"\n",
    "    logger.info(f\"Processing {len(data)} samples with {model_type} model\")\n",
    "    \n",
    "    if not data:\n",
    "        raise ValueError(\"Empty data provided\")\n",
    "    \n",
    "    if model_type not in ['linear', 'neural', 'tree']:\n",
    "        raise ValueError(f\"Unsupported model type: {model_type}\")\n",
    "    \n",
    "    # Simulate processing\n",
    "    processed = [x * 2 for x in data]\n",
    "    \n",
    "    logger.info(f\"Successfully processed data. Output size: {len(processed)}\")\n",
    "    return processed\n",
    "\n",
    "# Demonstration of debugging\n",
    "print(\"=== Debugging Examples ===\")\n",
    "\n",
    "# Successful case\n",
    "try:\n",
    "    result = process_ml_data([1, 2, 3, 4, 5], 'neural')\n",
    "    print(f\"Success: {result}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Error case - empty data\n",
    "try:\n",
    "    result = process_ml_data([], 'linear')\n",
    "except Exception as e:\n",
    "    print(f\"Caught expected error: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Error case - invalid model type\n",
    "try:\n",
    "    result = process_ml_data([1, 2, 3], 'quantum')\n",
    "except Exception as e:\n",
    "    print(f\"Caught expected error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Advanced Debugging with pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "\n",
    "def complex_ml_function(X, y, learning_rate=0.01):\n",
    "    \"\"\"\n",
    "    Simplified gradient descent implementation for debugging demo.\n",
    "    \"\"\"\n",
    "    # Initialize weights\n",
    "    weights = [0.0] * len(X[0]) if X else []\n",
    "    \n",
    "    print(f\"Starting with weights: {weights}\")\n",
    "    \n",
    "    for epoch in range(3):  # Simplified to 3 epochs\n",
    "        total_error = 0\n",
    "        \n",
    "        for i, (features, target) in enumerate(zip(X, y)):\n",
    "            # Simple prediction: dot product\n",
    "            prediction = sum(w * f for w, f in zip(weights, features))\n",
    "            \n",
    "            # Calculate error\n",
    "            error = target - prediction\n",
    "            total_error += error ** 2\n",
    "            \n",
    "            # Update weights (simplified gradient descent)\n",
    "            for j in range(len(weights)):\n",
    "                weights[j] += learning_rate * error * features[j]\n",
    "        \n",
    "        avg_error = total_error / len(X)\n",
    "        print(f\"Epoch {epoch + 1}: Average Error = {avg_error:.4f}\")\n",
    "        print(f\"Current weights: {[round(w, 4) for w in weights]}\")\n",
    "        \n",
    "        # Set breakpoint here for debugging (commented out for notebook)\n",
    "        # pdb.set_trace()  # Uncomment this line to debug interactively\n",
    "    \n",
    "    return weights\n",
    "\n",
    "# Example usage\n",
    "sample_X = [[1, 2], [2, 3], [3, 4], [4, 5]]\n",
    "sample_y = [3, 5, 7, 9]  # y = x1 + x2\n",
    "\n",
    "print(\"=== Debugging Complex ML Function ===\")\n",
    "final_weights = complex_ml_function(sample_X, sample_y, learning_rate=0.1)\n",
    "print(f\"\\nFinal weights: {[round(w, 4) for w in final_weights]}\")\n",
    "\n",
    "# PDB commands reference (for interactive debugging):\n",
    "print(\"\\n=== PDB Commands Reference ===\")\n",
    "print(\"n (next): Execute next line\")\n",
    "print(\"s (step): Step into function calls\")\n",
    "print(\"c (continue): Continue execution\")\n",
    "print(\"l (list): Show current code\")\n",
    "print(\"p <var>: Print variable value\")\n",
    "print(\"pp <var>: Pretty print variable\")\n",
    "print(\"w (where): Show stack trace\")\n",
    "print(\"q (quit): Quit debugger\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Property-Based Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Property-based testing example (simplified without external libraries)\n",
    "import random\n",
    "\n",
    "def property_test_text_preprocessing():\n",
    "    \"\"\"\n",
    "    Property-based test for text preprocessing function.\n",
    "    Tests that preprocessing is idempotent (applying twice gives same result).\n",
    "    \"\"\"\n",
    "    test_cases = 0\n",
    "    failures = []\n",
    "    \n",
    "    # Generate random test cases\n",
    "    for _ in range(100):\n",
    "        test_cases += 1\n",
    "        \n",
    "        # Generate random string with various whitespace patterns\n",
    "        words = ['hello', 'world', 'machine', 'learning', 'NLP', 'TEST']\n",
    "        text_parts = random.choices(words, k=random.randint(1, 5))\n",
    "        \n",
    "        # Add random whitespace\n",
    "        text = ''\n",
    "        for part in text_parts:\n",
    "            text += ' ' * random.randint(0, 3) + part + ' ' * random.randint(0, 3)\n",
    "        \n",
    "        try:\n",
    "            # Property: preprocessing should be idempotent\n",
    "            processed_once = preprocess_text(text)\n",
    "            processed_twice = preprocess_text(processed_once)\n",
    "            \n",
    "            if processed_once != processed_twice:\n",
    "                failures.append({\n",
    "                    'input': repr(text),\n",
    "                    'first_result': repr(processed_once),\n",
    "                    'second_result': repr(processed_twice)\n",
    "                })\n",
    "        \n",
    "        except Exception as e:\n",
    "            failures.append({\n",
    "                'input': repr(text),\n",
    "                'error': str(e)\n",
    "            })\n",
    "    \n",
    "    return test_cases, failures\n",
    "\n",
    "def property_test_confusion_matrix():\n",
    "    \"\"\"\n",
    "    Property-based test for confusion matrix calculation.\n",
    "    Tests that tp + tn + fp + fn equals total samples.\n",
    "    \"\"\"\n",
    "    test_cases = 0\n",
    "    failures = []\n",
    "    \n",
    "    for _ in range(50):\n",
    "        test_cases += 1\n",
    "        \n",
    "        # Generate random binary classification data\n",
    "        n_samples = random.randint(10, 100)\n",
    "        y_true = [random.randint(0, 1) for _ in range(n_samples)]\n",
    "        y_pred = [random.randint(0, 1) for _ in range(n_samples)]\n",
    "        \n",
    "        try:\n",
    "            cm = confusion_matrix_binary(y_true, y_pred)\n",
    "            total_from_cm = cm['tp'] + cm['tn'] + cm['fp'] + cm['fn']\n",
    "            \n",
    "            # Property: sum of confusion matrix should equal total samples\n",
    "            if total_from_cm != n_samples:\n",
    "                failures.append({\n",
    "                    'n_samples': n_samples,\n",
    "                    'cm_total': total_from_cm,\n",
    "                    'cm': cm\n",
    "                })\n",
    "        \n",
    "        except Exception as e:\n",
    "            failures.append({\n",
    "                'n_samples': n_samples,\n",
    "                'error': str(e)\n",
    "            })\n",
    "    \n",
    "    return test_cases, failures\n",
    "\n",
    "# Run property-based tests\n",
    "print(\"=== Property-Based Testing ===\")\n",
    "\n",
    "# Test text preprocessing idempotence\n",
    "cases, failures = property_test_text_preprocessing()\n",
    "print(f\"Text preprocessing test: {cases} cases, {len(failures)} failures\")\n",
    "if failures:\n",
    "    print(\"Failures:\", failures[:3])  # Show first 3 failures\n",
    "\n",
    "# Test confusion matrix properties\n",
    "cases, failures = property_test_confusion_matrix()\n",
    "print(f\"Confusion matrix test: {cases} cases, {len(failures)} failures\")\n",
    "if failures:\n",
    "    print(\"Failures:\", failures[:3])  # Show first 3 failures\n",
    "\n",
    "print(\"\\nProperty-based testing helps discover edge cases and ensure invariants!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Integration Testing Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integration testing for an ML pipeline\n",
    "\n",
    "class SimpleMLPipeline:\n",
    "    \"\"\"Simple ML pipeline for integration testing.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.is_trained = False\n",
    "        self.model_weights = None\n",
    "        self.preprocessing_params = None\n",
    "    \n",
    "    def preprocess(self, texts):\n",
    "        \"\"\"Preprocess text data.\"\"\"\n",
    "        if not isinstance(texts, list):\n",
    "            raise TypeError(\"Input must be a list\")\n",
    "        \n",
    "        # Simple preprocessing: lowercase and tokenize\n",
    "        processed = []\n",
    "        vocab = set()\n",
    "        \n",
    "        for text in texts:\n",
    "            cleaned = preprocess_text(text)\n",
    "            tokens = cleaned.split()\n",
    "            processed.append(tokens)\n",
    "            vocab.update(tokens)\n",
    "        \n",
    "        # Create simple vocabulary mapping\n",
    "        self.preprocessing_params = {word: i for i, word in enumerate(sorted(vocab))}\n",
    "        \n",
    "        return processed\n",
    "    \n",
    "    def vectorize(self, tokenized_texts):\n",
    "        \"\"\"Convert tokens to vectors.\"\"\"\n",
    "        if not self.preprocessing_params:\n",
    "            raise ValueError(\"Must call preprocess first\")\n",
    "        \n",
    "        vectors = []\n",
    "        vocab_size = len(self.preprocessing_params)\n",
    "        \n",
    "        for tokens in tokenized_texts:\n",
    "            vector = [0] * vocab_size\n",
    "            for token in tokens:\n",
    "                if token in self.preprocessing_params:\n",
    "                    vector[self.preprocessing_params[token]] = 1\n",
    "            vectors.append(vector)\n",
    "        \n",
    "        return vectors\n",
    "    \n",
    "    def train(self, X, y, epochs=3):\n",
    "        \"\"\"Train the model.\"\"\"\n",
    "        if not X or not y:\n",
    "            raise ValueError(\"Training data cannot be empty\")\n",
    "        \n",
    "        if len(X) != len(y):\n",
    "            raise ValueError(\"X and y must have same length\")\n",
    "        \n",
    "        # Simple training: initialize weights\n",
    "        feature_count = len(X[0]) if X else 0\n",
    "        self.model_weights = [0.1] * feature_count\n",
    "        \n",
    "        # Simulate training\n",
    "        for epoch in range(epochs):\n",
    "            for features, label in zip(X, y):\n",
    "                prediction = sum(w * f for w, f in zip(self.model_weights, features))\n",
    "                error = label - prediction\n",
    "                \n",
    "                # Update weights\n",
    "                for i in range(len(self.model_weights)):\n",
    "                    self.model_weights[i] += 0.01 * error * features[i]\n",
    "        \n",
    "        self.is_trained = True\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions.\"\"\"\n",
    "        if not self.is_trained:\n",
    "            raise ValueError(\"Model must be trained first\")\n",
    "        \n",
    "        predictions = []\n",
    "        for features in X:\n",
    "            prediction = sum(w * f for w, f in zip(self.model_weights, features))\n",
    "            predictions.append(1 if prediction > 0.5 else 0)\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "class TestMLPipelineIntegration(unittest.TestCase):\n",
    "    \"\"\"Integration tests for the ML pipeline.\"\"\"\n",
    "    \n",
    "    def setUp(self):\n",
    "        self.pipeline = SimpleMLPipeline()\n",
    "        self.sample_texts = [\n",
    "            'This is positive',\n",
    "            'This is negative', \n",
    "            'Great product',\n",
    "            'Bad experience'\n",
    "        ]\n",
    "        self.sample_labels = [1, 0, 1, 0]\n",
    "    \n",
    "    def test_full_pipeline_integration(self):\n",
    "        \"\"\"Test the entire ML pipeline from text to prediction.\"\"\"\n",
    "        # Step 1: Preprocess\n",
    "        tokenized = self.pipeline.preprocess(self.sample_texts)\n",
    "        self.assertEqual(len(tokenized), len(self.sample_texts))\n",
    "        self.assertIsNotNone(self.pipeline.preprocessing_params)\n",
    "        \n",
    "        # Step 2: Vectorize\n",
    "        vectors = self.pipeline.vectorize(tokenized)\n",
    "        self.assertEqual(len(vectors), len(self.sample_texts))\n",
    "        self.assertTrue(all(isinstance(v, list) for v in vectors))\n",
    "        \n",
    "        # Step 3: Train\n",
    "        self.pipeline.train(vectors, self.sample_labels)\n",
    "        self.assertTrue(self.pipeline.is_trained)\n",
    "        self.assertIsNotNone(self.pipeline.model_weights)\n",
    "        \n",
    "        # Step 4: Predict\n",
    "        predictions = self.pipeline.predict(vectors)\n",
    "        self.assertEqual(len(predictions), len(self.sample_texts))\n",
    "        self.assertTrue(all(p in [0, 1] for p in predictions))\n",
    "        \n",
    "        print(f\"Pipeline test completed successfully!\")\n",
    "        print(f\"Original texts: {self.sample_texts}\")\n",
    "        print(f\"True labels: {self.sample_labels}\")\n",
    "        print(f\"Predictions: {predictions}\")\n",
    "    \n",
    "    def test_pipeline_with_new_data(self):\n",
    "        \"\"\"Test pipeline with new unseen data.\"\"\"\n",
    "        # Train pipeline\n",
    "        tokenized = self.pipeline.preprocess(self.sample_texts)\n",
    "        vectors = self.pipeline.vectorize(tokenized)\n",
    "        self.pipeline.train(vectors, self.sample_labels)\n",
    "        \n",
    "        # Test with new data\n",
    "        new_texts = ['Amazing quality', 'Terrible service']\n",
    "        new_tokenized = self.pipeline.preprocess(new_texts)\n",
    "        new_vectors = self.pipeline.vectorize(new_tokenized)\n",
    "        predictions = self.pipeline.predict(new_vectors)\n",
    "        \n",
    "        self.assertEqual(len(predictions), 2)\n",
    "        print(f\"New data predictions: {dict(zip(new_texts, predictions))}\")\n",
    "\n",
    "# Run integration tests\n",
    "test_output = StringIO()\n",
    "runner = unittest.TextTestRunner(stream=test_output, verbosity=2)\n",
    "suite = unittest.TestLoader().loadTestsFromTestCase(TestMLPipelineIntegration)\n",
    "result = runner.run(suite)\n",
    "\n",
    "print(test_output.getvalue())\n",
    "print(f\"\\nIntegration Tests - Run: {result.testsRun}, Failures: {len(result.failures)}, Errors: {len(result.errors)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Performance Testing and Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import cProfile\n",
    "import pstats\n",
    "from io import StringIO\n",
    "\n",
    "def performance_test_decorator(func):\n",
    "    \"\"\"Decorator to measure function performance.\"\"\"\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        execution_time = end_time - start_time\n",
    "        print(f\"{func.__name__} executed in {execution_time:.4f} seconds\")\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "@performance_test_decorator\n",
    "def inefficient_text_processing(texts):\n",
    "    \"\"\"Inefficient text processing for performance testing.\"\"\"\n",
    "    result = []\n",
    "    for text in texts:\n",
    "        # Inefficient: multiple string operations\n",
    "        processed = text.lower()\n",
    "        processed = processed.strip()\n",
    "        processed = ' '.join(processed.split())\n",
    "        \n",
    "        # Inefficient: nested loops\n",
    "        word_count = 0\n",
    "        for word in processed.split():\n",
    "            for char in word:\n",
    "                if char.isalpha():\n",
    "                    word_count += 0.001  # Simulate work\n",
    "        \n",
    "        result.append(processed)\n",
    "    return result\n",
    "\n",
    "@performance_test_decorator\n",
    "def efficient_text_processing(texts):\n",
    "    \"\"\"More efficient text processing.\"\"\"\n",
    "    result = []\n",
    "    for text in texts:\n",
    "        # More efficient: single pass\n",
    "        processed = ' '.join(text.lower().split())\n",
    "        result.append(processed)\n",
    "    return result\n",
    "\n",
    "# Performance comparison\n",
    "print(\"=== Performance Testing ===\")\n",
    "\n",
    "# Generate test data\n",
    "test_texts = [f\"Sample text number {i} with some content\" for i in range(1000)]\n",
    "\n",
    "print(\"Testing inefficient version:\")\n",
    "result1 = inefficient_text_processing(test_texts)\n",
    "\n",
    "print(\"\\nTesting efficient version:\")\n",
    "result2 = efficient_text_processing(test_texts)\n",
    "\n",
    "# Verify results are the same\n",
    "print(f\"\\nResults match: {result1 == result2}\")\n",
    "\n",
    "# Profiling example\n",
    "def profile_function(func, *args, **kwargs):\n",
    "    \"\"\"Profile a function and return statistics.\"\"\"\n",
    "    profiler = cProfile.Profile()\n",
    "    profiler.enable()\n",
    "    \n",
    "    result = func(*args, **kwargs)\n",
    "    \n",
    "    profiler.disable()\n",
    "    \n",
    "    # Get statistics\n",
    "    stats_stream = StringIO()\n",
    "    stats = pstats.Stats(profiler, stream=stats_stream)\n",
    "    stats.sort_stats('cumulative')\n",
    "    stats.print_stats(10)  # Show top 10 functions\n",
    "    \n",
    "    return result, stats_stream.getvalue()\n",
    "\n",
    "print(\"\\n=== Profiling Results ===\")\n",
    "result, profile_output = profile_function(inefficient_text_processing, test_texts[:100])\n",
    "print(profile_output[:500] + \"...\" if len(profile_output) > 500 else profile_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Test Coverage Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple test coverage tracking (manual implementation)\n",
    "class CoverageTracker:\n",
    "    \"\"\"Simple test coverage tracker.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.executed_lines = set()\n",
    "        self.total_lines = set()\n",
    "    \n",
    "    def mark_line(self, line_id):\n",
    "        \"\"\"Mark a line as executed.\"\"\"\n",
    "        self.executed_lines.add(line_id)\n",
    "        self.total_lines.add(line_id)\n",
    "    \n",
    "    def add_total_line(self, line_id):\n",
    "        \"\"\"Add a line to total count without marking as executed.\"\"\"\n",
    "        self.total_lines.add(line_id)\n",
    "    \n",
    "    def get_coverage(self):\n",
    "        \"\"\"Get coverage percentage.\"\"\"\n",
    "        if not self.total_lines:\n",
    "            return 0.0\n",
    "        return (len(self.executed_lines) / len(self.total_lines)) * 100\n",
    "    \n",
    "    def get_uncovered_lines(self):\n",
    "        \"\"\"Get lines that weren't executed.\"\"\"\n",
    "        return self.total_lines - self.executed_lines\n",
    "\n",
    "# Instrumented function for coverage tracking\n",
    "coverage = CoverageTracker()\n",
    "\n",
    "def instrumented_sentiment_analyzer(text):\n",
    "    \"\"\"Simple sentiment analyzer with coverage tracking.\"\"\"\n",
    "    coverage.add_total_line('func_start')\n",
    "    coverage.mark_line('func_start')\n",
    "    \n",
    "    if not isinstance(text, str):\n",
    "        coverage.add_total_line('type_error')\n",
    "        coverage.mark_line('type_error')\n",
    "        raise TypeError(\"Input must be string\")\n",
    "    \n",
    "    coverage.add_total_line('empty_check')\n",
    "    coverage.mark_line('empty_check')\n",
    "    if not text.strip():\n",
    "        coverage.add_total_line('empty_return')\n",
    "        coverage.mark_line('empty_return')\n",
    "        return 'neutral'\n",
    "    \n",
    "    coverage.add_total_line('preprocessing')\n",
    "    coverage.mark_line('preprocessing')\n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    # Simple sentiment keywords\n",
    "    positive_words = ['good', 'great', 'excellent', 'amazing', 'love']\n",
    "    negative_words = ['bad', 'terrible', 'awful', 'hate', 'horrible']\n",
    "    \n",
    "    coverage.add_total_line('scoring')\n",
    "    coverage.mark_line('scoring')\n",
    "    positive_score = sum(1 for word in positive_words if word in text_lower)\n",
    "    negative_score = sum(1 for word in negative_words if word in text_lower)\n",
    "    \n",
    "    coverage.add_total_line('decision')\n",
    "    coverage.mark_line('decision')\n",
    "    if positive_score > negative_score:\n",
    "        coverage.add_total_line('positive_return')\n",
    "        coverage.mark_line('positive_return')\n",
    "        return 'positive'\n",
    "    elif negative_score > positive_score:\n",
    "        coverage.add_total_line('negative_return')\n",
    "        coverage.mark_line('negative_return')\n",
    "        return 'negative'\n",
    "    else:\n",
    "        coverage.add_total_line('neutral_return')\n",
    "        coverage.mark_line('neutral_return')\n",
    "        return 'neutral'\n",
    "\n",
    "class TestSentimentWithCoverage(unittest.TestCase):\n",
    "    \"\"\"Tests with coverage tracking.\"\"\"\n",
    "    \n",
    "    def test_positive_sentiment(self):\n",
    "        result = instrumented_sentiment_analyzer(\"This is great!\")\n",
    "        self.assertEqual(result, 'positive')\n",
    "    \n",
    "    def test_negative_sentiment(self):\n",
    "        result = instrumented_sentiment_analyzer(\"This is terrible!\")\n",
    "        self.assertEqual(result, 'negative')\n",
    "    \n",
    "    def test_neutral_sentiment(self):\n",
    "        result = instrumented_sentiment_analyzer(\"This is okay\")\n",
    "        self.assertEqual(result, 'neutral')\n",
    "    \n",
    "    def test_empty_text(self):\n",
    "        result = instrumented_sentiment_analyzer(\"  \")\n",
    "        self.assertEqual(result, 'neutral')\n",
    "    \n",
    "    # Note: We're not testing the TypeError case\n",
    "\n",
    "# Run tests with coverage\n",
    "print(\"=== Test Coverage Analysis ===\")\n",
    "\n",
    "# Reset coverage\n",
    "coverage = CoverageTracker()\n",
    "\n",
    "# Add all possible lines to total count\n",
    "for line_id in ['func_start', 'type_error', 'empty_check', 'empty_return', \n",
    "                'preprocessing', 'scoring', 'decision', 'positive_return', \n",
    "                'negative_return', 'neutral_return']:\n",
    "    coverage.add_total_line(line_id)\n",
    "\n",
    "# Run tests\n",
    "test_output = StringIO()\n",
    "runner = unittest.TextTestRunner(stream=test_output, verbosity=1)\n",
    "suite = unittest.TestLoader().loadTestsFromTestCase(TestSentimentWithCoverage)\n",
    "result = runner.run(suite)\n",
    "\n",
    "print(f\"Tests run: {result.testsRun}\")\n",
    "print(f\"Test coverage: {coverage.get_coverage():.1f}%\")\n",
    "print(f\"Executed lines: {len(coverage.executed_lines)}\")\n",
    "print(f\"Total lines: {len(coverage.total_lines)}\")\n",
    "print(f\"Uncovered lines: {coverage.get_uncovered_lines()}\")\n",
    "\n",
    "print(\"\\n=== Coverage Report ===\")\n",
    "print(\"Lines not covered by tests:\")\n",
    "for line in coverage.get_uncovered_lines():\n",
    "    print(f\"  - {line}\")\n",
    "\n",
    "print(\"\\nTo improve coverage, add tests for:\")\n",
    "print(\"  - TypeError handling (invalid input type)\")\n",
    "print(\"  - Additional edge cases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Testing Best Practices Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of testing best practices for ML/NLP projects\n",
    "\n",
    "print(\"=== Testing Best Practices for ML/NLP ===\")\n",
    "\n",
    "best_practices = {\n",
    "    \"Test Structure\": [\n",
    "        \"Use descriptive test names\",\n",
    "        \"Follow AAA pattern: Arrange, Act, Assert\",\n",
    "        \"One assertion per test when possible\",\n",
    "        \"Use setUp and tearDown for fixtures\"\n",
    "    ],\n",
    "    \n",
    "    \"ML-Specific Testing\": [\n",
    "        \"Test data preprocessing pipelines\",\n",
    "        \"Validate model input/output shapes\",\n",
    "        \"Test edge cases in data handling\",\n",
    "        \"Mock external dependencies (APIs, databases)\",\n",
    "        \"Test model serialization/deserialization\"\n",
    "    ],\n",
    "    \n",
    "    \"Test Types\": [\n",
    "        \"Unit tests: Individual functions\",\n",
    "        \"Integration tests: Component interaction\",\n",
    "        \"End-to-end tests: Full pipeline\",\n",
    "        \"Performance tests: Speed and memory\",\n",
    "        \"Property-based tests: Invariants\"\n",
    "    ],\n",
    "    \n",
    "    \"Data Testing\": [\n",
    "        \"Validate data schemas and types\",\n",
    "        \"Test data cleaning functions\",\n",
    "        \"Check for data leakage in splits\",\n",
    "        \"Verify feature engineering correctness\",\n",
    "        \"Test handling of missing values\"\n",
    "    ],\n",
    "    \n",
    "    \"Debugging Tips\": [\n",
    "        \"Use logging instead of print statements\",\n",
    "        \"Set up proper error handling\",\n",
    "        \"Use debugger (pdb) for complex issues\",\n",
    "        \"Profile performance bottlenecks\",\n",
    "        \"Write reproducible test cases\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "for category, practices in best_practices.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    for practice in practices:\n",
    "        print(f\"  • {practice}\")\n",
    "\n",
    "print(\"\\n=== Common ML Testing Patterns ===\")\n",
    "\n",
    "patterns = [\n",
    "    \"✓ Test with small synthetic datasets first\",\n",
    "    \"✓ Use fixtures for consistent test data\", \n",
    "    \"✓ Mock external services and APIs\",\n",
    "    \"✓ Test boundary conditions and edge cases\",\n",
    "    \"✓ Verify numerical stability and precision\",\n",
    "    \"✓ Test model behavior with different data distributions\",\n",
    "    \"✓ Validate that training improves performance\",\n",
    "    \"✓ Test model persistence and loading\",\n",
    "    \"✓ Use property-based testing for mathematical functions\",\n",
    "    \"✓ Test preprocessing consistency across train/test\"\n",
    "]\n",
    "\n",
    "for pattern in patterns:\n",
    "    print(pattern)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Remember: Good tests make refactoring safe and debugging easier!\")\n",
    "print(\"Write tests first (TDD) when possible, and aim for high coverage.\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Exercises\n",
    "\n",
    "### Exercise 1: Write Unit Tests\n",
    "Create unit tests for a function that calculates TF-IDF scores for documents.\n",
    "\n",
    "### Exercise 2: Mock External API\n",
    "Write tests for a function that calls a translation API, using mocks to simulate API responses.\n",
    "\n",
    "### Exercise 3: Integration Testing\n",
    "Create integration tests for a complete text classification pipeline.\n",
    "\n",
    "### Exercise 4: Debug Complex Function\n",
    "Use debugging techniques to fix a buggy implementation of a recommendation algorithm.\n",
    "\n",
    "### Exercise 5: Performance Testing\n",
    "Compare the performance of different text similarity algorithms and write performance tests.\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Testing Philosophy**: Write tests first, test behavior not implementation\n",
    "2. **Test Types**: Unit, integration, end-to-end, and performance tests all have their place\n",
    "3. **ML Testing**: Special considerations for data pipelines, model validation, and reproducibility\n",
    "4. **Debugging**: Use systematic approaches, logging, and proper tools\n",
    "5. **Coverage**: Aim for high test coverage but focus on critical paths\n",
    "6. **Automation**: Integrate testing into your development workflow\n",
    "\n",
    "These testing and debugging skills are essential for building reliable ML and NLP systems!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}