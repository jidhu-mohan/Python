{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Performance and Memory Management\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this notebook, you will understand:\n",
    "- Memory management in Python\n",
    "- Performance optimization techniques\n",
    "- Profiling and benchmarking\n",
    "- Memory-efficient data structures\n",
    "- Garbage collection and memory leaks\n",
    "- Optimization for ML/NLP workloads\n",
    "\n",
    "## Why Performance Matters for ML/NLP\n",
    "- Large datasets require efficient memory usage\n",
    "- Training time optimization is crucial\n",
    "- Real-time inference needs fast execution\n",
    "- Resource-constrained environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Memory Management Fundamentals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gc\n",
    "import tracemalloc\n",
    "import psutil\n",
    "import os\n",
    "from memory_profiler import profile\n",
    "\n",
    "def get_memory_usage():\n",
    "    \"\"\"Get current memory usage of the process.\"\"\"\n",
    "    process = psutil.Process(os.getpid())\n",
    "    memory_info = process.memory_info()\n",
    "    return memory_info.rss / 1024 / 1024  # MB\n",
    "\n",
    "def get_object_size(obj):\n",
    "    \"\"\"Get the size of an object in bytes.\"\"\"\n",
    "    return sys.getsizeof(obj)\n",
    "\n",
    "print(\"=== Memory Management Basics ===\")\n",
    "print(f\"Current memory usage: {get_memory_usage():.2f} MB\")\n",
    "\n",
    "# Memory usage of different data types\n",
    "data_types = {\n",
    "    'int': 42,\n",
    "    'float': 3.14159,\n",
    "    'string': \"Hello, World!\",\n",
    "    'list': [1, 2, 3, 4, 5],\n",
    "    'dict': {'a': 1, 'b': 2, 'c': 3},\n",
    "    'tuple': (1, 2, 3, 4, 5),\n",
    "    'set': {1, 2, 3, 4, 5}\n",
    "}\n",
    "\n",
    "print(\"\\nMemory usage by data type:\")\n",
    "for name, obj in data_types.items():\n",
    "    size = get_object_size(obj)\n",
    "    print(f\"{name:8}: {size:4} bytes\")\n",
    "\n",
    "# Memory growth with list size\n",
    "print(\"\\nMemory usage growth with list size:\")\n",
    "for size in [10, 100, 1000, 10000]:\n",
    "    lst = list(range(size))\n",
    "    memory_size = get_object_size(lst)\n",
    "    print(f\"List of {size:5} elements: {memory_size:8} bytes ({memory_size/size:.2f} bytes per element)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Memory-Efficient Data Structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import array\n",
    "from collections import deque, defaultdict, Counter\n",
    "import numpy as np\n",
    "\n",
    "# Compare memory usage of different sequence types\n",
    "def compare_sequence_memory():\n",
    "    \"\"\"Compare memory usage of different sequence types.\"\"\"\n",
    "    n = 10000\n",
    "    \n",
    "    # Python list\n",
    "    py_list = list(range(n))\n",
    "    \n",
    "    # Python array (typed)\n",
    "    py_array = array.array('i', range(n))  # 'i' for integers\n",
    "    \n",
    "    # NumPy array\n",
    "    np_array = np.arange(n, dtype=np.int32)\n",
    "    \n",
    "    # Tuple\n",
    "    py_tuple = tuple(range(n))\n",
    "    \n",
    "    sequences = {\n",
    "        'Python list': py_list,\n",
    "        'Python array': py_array,\n",
    "        'NumPy array': np_array,\n",
    "        'Tuple': py_tuple\n",
    "    }\n",
    "    \n",
    "    print(f\"Memory usage for {n} integers:\")\n",
    "    for name, seq in sequences.items():\n",
    "        size = get_object_size(seq)\n",
    "        if hasattr(seq, 'nbytes'):  # NumPy array\n",
    "            size = seq.nbytes\n",
    "        print(f\"{name:12}: {size:8} bytes ({size/n:.2f} bytes per element)\")\n",
    "\n",
    "compare_sequence_memory()\n",
    "\n",
    "print(\"\\n=== Generator vs List Memory Usage ===\")\n",
    "\n",
    "def memory_efficient_processing():\n",
    "    \"\"\"Demonstrate memory-efficient processing with generators.\"\"\"\n",
    "    \n",
    "    # Memory-heavy approach: create full list\n",
    "    def process_with_list(n):\n",
    "        data = [i**2 for i in range(n)]  # Create full list\n",
    "        return sum(x for x in data if x % 2 == 0)\n",
    "    \n",
    "    # Memory-efficient approach: use generator\n",
    "    def process_with_generator(n):\n",
    "        data = (i**2 for i in range(n))  # Generator expression\n",
    "        return sum(x for x in data if x % 2 == 0)\n",
    "    \n",
    "    # Compare memory usage\n",
    "    n = 100000\n",
    "    \n",
    "    print(f\"Processing {n} numbers:\")\n",
    "    \n",
    "    # Measure list approach\n",
    "    initial_memory = get_memory_usage()\n",
    "    result1 = process_with_list(n)\n",
    "    list_memory = get_memory_usage()\n",
    "    \n",
    "    # Force garbage collection\n",
    "    gc.collect()\n",
    "    \n",
    "    # Measure generator approach  \n",
    "    gc_memory = get_memory_usage()\n",
    "    result2 = process_with_generator(n)\n",
    "    generator_memory = get_memory_usage()\n",
    "    \n",
    "    print(f\"List approach memory increase: {list_memory - initial_memory:.2f} MB\")\n",
    "    print(f\"Generator approach memory increase: {generator_memory - gc_memory:.2f} MB\")\n",
    "    print(f\"Results match: {result1 == result2}\")\n",
    "    print(f\"Result: {result1}\")\n",
    "\n",
    "memory_efficient_processing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Performance Profiling and Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import timeit\n",
    "import cProfile\n",
    "import pstats\n",
    "from io import StringIO\n",
    "\n",
    "# Performance measurement decorator\n",
    "def benchmark(func):\n",
    "    \"\"\"Decorator to benchmark function execution time.\"\"\"\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.perf_counter()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.perf_counter()\n",
    "        execution_time = end_time - start_time\n",
    "        print(f\"{func.__name__}: {execution_time:.6f} seconds\")\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "# Different approaches to text processing\n",
    "@benchmark\n",
    "def string_concatenation_slow(words):\n",
    "    \"\"\"Slow string concatenation.\"\"\"\n",
    "    result = \"\"\n",
    "    for word in words:\n",
    "        result += word + \" \"\n",
    "    return result.strip()\n",
    "\n",
    "@benchmark \n",
    "def string_concatenation_fast(words):\n",
    "    \"\"\"Fast string concatenation.\"\"\"\n",
    "    return \" \".join(words)\n",
    "\n",
    "@benchmark\n",
    "def list_comprehension_approach(words):\n",
    "    \"\"\"Using list comprehension.\"\"\"\n",
    "    return \" \".join([word for word in words])\n",
    "\n",
    "@benchmark\n",
    "def generator_approach(words):\n",
    "    \"\"\"Using generator expression.\"\"\"\n",
    "    return \" \".join(word for word in words)\n",
    "\n",
    "# Test with sample data\n",
    "print(\"=== String Processing Performance Comparison ===\")\n",
    "sample_words = [f\"word{i}\" for i in range(1000)]\n",
    "\n",
    "result1 = string_concatenation_slow(sample_words)\n",
    "result2 = string_concatenation_fast(sample_words)\n",
    "result3 = list_comprehension_approach(sample_words)\n",
    "result4 = generator_approach(sample_words)\n",
    "\n",
    "print(f\"All results equal: {result1 == result2 == result3 == result4}\")\n",
    "\n",
    "# Using timeit for more precise measurements\n",
    "print(\"\\n=== Precise Timing with timeit ===\")\n",
    "\n",
    "setup_code = \"words = [f'word{i}' for i in range(100)]\"\n",
    "\n",
    "# Test different approaches\n",
    "approaches = {\n",
    "    'String concat': \"''.join(words)\",\n",
    "    'Plus operator': \"result = ''; [result := result + word + ' ' for word in words]; result.strip()\",\n",
    "    'Format string': \"' '.join('{}' for _ in words).format(*words)\",\n",
    "    'Join method': \"' '.join(words)\"\n",
    "}\n",
    "\n",
    "for name, code in approaches.items():\n",
    "    try:\n",
    "        time_taken = timeit.timeit(code, setup=setup_code, number=1000)\n",
    "        print(f\"{name:15}: {time_taken:.6f} seconds (1000 iterations)\")\n",
    "    except:\n",
    "        print(f\"{name:15}: Error in execution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Memory Profiling with tracemalloc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tracemalloc\n",
    "import linecache\n",
    "\n",
    "def memory_intensive_function():\n",
    "    \"\"\"Function that uses significant memory.\"\"\"\n",
    "    # Create large data structures\n",
    "    large_list = list(range(100000))  # Line that will show up in trace\n",
    "    large_dict = {i: f\"value_{i}\" for i in range(50000)}  # Another memory-heavy line\n",
    "    \n",
    "    # Some processing\n",
    "    processed_data = [x * 2 for x in large_list[:10000]]\n",
    "    \n",
    "    return len(processed_data)\n",
    "\n",
    "def analyze_memory_usage():\n",
    "    \"\"\"Analyze memory usage with tracemalloc.\"\"\"\n",
    "    print(\"=== Memory Profiling with tracemalloc ===\")\n",
    "    \n",
    "    # Start tracing\n",
    "    tracemalloc.start()\n",
    "    \n",
    "    # Get initial snapshot\n",
    "    snapshot1 = tracemalloc.take_snapshot()\n",
    "    \n",
    "    # Run memory-intensive function\n",
    "    result = memory_intensive_function()\n",
    "    \n",
    "    # Get final snapshot\n",
    "    snapshot2 = tracemalloc.take_snapshot()\n",
    "    \n",
    "    # Compare snapshots\n",
    "    top_stats = snapshot2.compare_to(snapshot1, 'lineno')\n",
    "    \n",
    "    print(f\"Function result: {result}\")\n",
    "    print(\"\\nTop 5 memory allocations:\")\n",
    "    \n",
    "    for index, stat in enumerate(top_stats[:5]):\n",
    "        print(f\"{index + 1}. {stat}\")\n",
    "        \n",
    "        # Show the actual code line\n",
    "        frame = stat.traceback.format()[-1]\n",
    "        print(f\"   Code: {frame.strip()}\")\n",
    "    \n",
    "    # Get current memory statistics\n",
    "    current, peak = tracemalloc.get_traced_memory()\n",
    "    print(f\"\\nCurrent memory usage: {current / 1024 / 1024:.2f} MB\")\n",
    "    print(f\"Peak memory usage: {peak / 1024 / 1024:.2f} MB\")\n",
    "    \n",
    "    # Stop tracing\n",
    "    tracemalloc.stop()\n",
    "\n",
    "analyze_memory_usage()\n",
    "\n",
    "# Memory leak detection\n",
    "def detect_memory_leaks():\n",
    "    \"\"\"Simple memory leak detection.\"\"\"\n",
    "    print(\"\\n=== Memory Leak Detection ===\")\n",
    "    \n",
    "    # Simulate a potential memory leak\n",
    "    global_cache = []  # This could cause memory leaks if not managed\n",
    "    \n",
    "    def leaky_function(size):\n",
    "        data = list(range(size))\n",
    "        global_cache.append(data)  # Adding to global cache\n",
    "        return len(data)\n",
    "    \n",
    "    # Monitor memory usage\n",
    "    initial_memory = get_memory_usage()\n",
    "    print(f\"Initial memory: {initial_memory:.2f} MB\")\n",
    "    \n",
    "    # Run function multiple times\n",
    "    for i in range(5):\n",
    "        leaky_function(10000)\n",
    "        current_memory = get_memory_usage()\n",
    "        print(f\"After iteration {i+1}: {current_memory:.2f} MB (+{current_memory - initial_memory:.2f} MB)\")\n",
    "    \n",
    "    print(f\"Global cache size: {len(global_cache)} items\")\n",
    "    \n",
    "    # Clean up\n",
    "    global_cache.clear()\n",
    "    gc.collect()\n",
    "    \n",
    "    final_memory = get_memory_usage()\n",
    "    print(f\"After cleanup: {final_memory:.2f} MB\")\n",
    "\n",
    "detect_memory_leaks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Optimization Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import lru_cache, reduce\n",
    "from operator import mul\n",
    "import itertools\n",
    "\n",
    "print(\"=== Optimization Techniques ===\")\n",
    "\n",
    "# 1. Memoization with lru_cache\n",
    "def fibonacci_slow(n):\n",
    "    \"\"\"Slow recursive fibonacci.\"\"\"\n",
    "    if n < 2:\n",
    "        return n\n",
    "    return fibonacci_slow(n-1) + fibonacci_slow(n-2)\n",
    "\n",
    "@lru_cache(maxsize=128)\n",
    "def fibonacci_fast(n):\n",
    "    \"\"\"Fast memoized fibonacci.\"\"\"\n",
    "    if n < 2:\n",
    "        return n\n",
    "    return fibonacci_fast(n-1) + fibonacci_fast(n-2)\n",
    "\n",
    "# Compare performance\n",
    "print(\"Fibonacci comparison (n=30):\")\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "result_slow = fibonacci_slow(30)\n",
    "slow_time = time.perf_counter() - start_time\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "result_fast = fibonacci_fast(30)\n",
    "fast_time = time.perf_counter() - start_time\n",
    "\n",
    "print(f\"Slow version: {slow_time:.6f} seconds\")\n",
    "print(f\"Fast version: {fast_time:.6f} seconds\")\n",
    "print(f\"Speedup: {slow_time / fast_time:.0f}x faster\")\n",
    "print(f\"Results match: {result_slow == result_fast}\")\n",
    "\n",
    "# 2. Using built-in functions vs loops\n",
    "print(\"\\n=== Built-in Functions vs Loops ===\")\n",
    "\n",
    "numbers = list(range(100000))\n",
    "\n",
    "@benchmark\n",
    "def manual_sum(numbers):\n",
    "    \"\"\"Manual summation with loop.\"\"\"\n",
    "    total = 0\n",
    "    for num in numbers:\n",
    "        total += num\n",
    "    return total\n",
    "\n",
    "@benchmark \n",
    "def builtin_sum(numbers):\n",
    "    \"\"\"Using built-in sum function.\"\"\"\n",
    "    return sum(numbers)\n",
    "\n",
    "@benchmark\n",
    "def numpy_sum(numbers):\n",
    "    \"\"\"Using NumPy sum.\"\"\"\n",
    "    arr = np.array(numbers)\n",
    "    return np.sum(arr)\n",
    "\n",
    "result1 = manual_sum(numbers)\n",
    "result2 = builtin_sum(numbers)\n",
    "result3 = int(numpy_sum(numbers))\n",
    "\n",
    "print(f\"All results equal: {result1 == result2 == result3}\")\n",
    "\n",
    "# 3. List comprehensions vs loops\n",
    "print(\"\\n=== List Comprehensions vs Loops ===\")\n",
    "\n",
    "@benchmark\n",
    "def process_with_loop(numbers):\n",
    "    \"\"\"Process numbers with explicit loop.\"\"\"\n",
    "    result = []\n",
    "    for num in numbers:\n",
    "        if num % 2 == 0:\n",
    "            result.append(num ** 2)\n",
    "    return result\n",
    "\n",
    "@benchmark\n",
    "def process_with_comprehension(numbers):\n",
    "    \"\"\"Process numbers with list comprehension.\"\"\"\n",
    "    return [num ** 2 for num in numbers if num % 2 == 0]\n",
    "\n",
    "@benchmark\n",
    "def process_with_filter_map(numbers):\n",
    "    \"\"\"Process numbers with filter and map.\"\"\"\n",
    "    return list(map(lambda x: x ** 2, filter(lambda x: x % 2 == 0, numbers)))\n",
    "\n",
    "test_numbers = list(range(10000))\n",
    "result1 = process_with_loop(test_numbers)\n",
    "result2 = process_with_comprehension(test_numbers)\n",
    "result3 = process_with_filter_map(test_numbers)\n",
    "\n",
    "print(f\"All results equal: {result1 == result2 == result3}\")\n",
    "print(f\"Result length: {len(result1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ML/NLP Specific Optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from collections import defaultdict, Counter\n",
    "import re\n",
    "\n",
    "print(\"=== ML/NLP Performance Optimizations ===\")\n",
    "\n",
    "# Sample text data for testing\n",
    "sample_texts = [\n",
    "    \"Natural language processing is fascinating and challenging.\",\n",
    "    \"Machine learning algorithms require careful optimization.\", \n",
    "    \"Text preprocessing can be computationally expensive.\",\n",
    "    \"Efficient implementations make a significant difference.\"\n",
    "] * 1000  # Multiply to create larger dataset\n",
    "\n",
    "# Text preprocessing optimization\n",
    "@benchmark\n",
    "def preprocess_text_slow(texts):\n",
    "    \"\"\"Slow text preprocessing.\"\"\"\n",
    "    processed = []\n",
    "    for text in texts:\n",
    "        # Multiple passes over the text\n",
    "        text = text.lower()\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "        text = ' '.join(text.split())  # Remove extra whitespace\n",
    "        words = text.split()\n",
    "        # Filter short words\n",
    "        words = [word for word in words if len(word) > 2]\n",
    "        processed.append(words)\n",
    "    return processed\n",
    "\n",
    "@benchmark\n",
    "def preprocess_text_fast(texts):\n",
    "    \"\"\"Optimized text preprocessing.\"\"\"\n",
    "    # Pre-compile regex for better performance\n",
    "    punct_pattern = re.compile(r'[^\\w\\s]')\n",
    "    \n",
    "    processed = []\n",
    "    for text in texts:\n",
    "        # Single pass with combined operations\n",
    "        text = punct_pattern.sub('', text.lower())\n",
    "        words = [word for word in text.split() if len(word) > 2]\n",
    "        processed.append(words)\n",
    "    return processed\n",
    "\n",
    "# Compare preprocessing approaches\n",
    "result1 = preprocess_text_slow(sample_texts[:100])  # Use smaller sample for slow version\n",
    "result2 = preprocess_text_fast(sample_texts)\n",
    "\n",
    "print(f\"Preprocessing results match (first 100): {result1 == result2[:100]}\")\n",
    "\n",
    "# Vectorization optimization\n",
    "print(\"\\n=== Text Vectorization Optimization ===\")\n",
    "\n",
    "@benchmark\n",
    "def create_bow_slow(texts, vocab):\n",
    "    \"\"\"Slow bag-of-words creation.\"\"\"\n",
    "    vectors = []\n",
    "    for text in texts:\n",
    "        vector = []\n",
    "        for word in vocab:\n",
    "            count = text.count(word)\n",
    "            vector.append(count)\n",
    "        vectors.append(vector)\n",
    "    return vectors\n",
    "\n",
    "@benchmark\n",
    "def create_bow_fast(texts, vocab):\n",
    "    \"\"\"Fast bag-of-words creation using Counter.\"\"\"\n",
    "    vocab_set = set(vocab)\n",
    "    vectors = []\n",
    "    \n",
    "    for text in texts:\n",
    "        word_counts = Counter(text)\n",
    "        vector = [word_counts.get(word, 0) for word in vocab]\n",
    "        vectors.append(vector)\n",
    "    return vectors\n",
    "\n",
    "# Create vocabulary from processed text\n",
    "processed_sample = preprocess_text_fast(sample_texts[:100])\n",
    "all_words = set()\n",
    "for words in processed_sample:\n",
    "    all_words.update(words)\n",
    "vocab = sorted(list(all_words))[:50]  # Use top 50 words\n",
    "\n",
    "print(f\"Vocabulary size: {len(vocab)}\")\n",
    "print(f\"Sample size: {len(processed_sample)}\")\n",
    "\n",
    "vectors1 = create_bow_slow(processed_sample[:20], vocab)  # Smaller sample for slow version\n",
    "vectors2 = create_bow_fast(processed_sample, vocab)\n",
    "\n",
    "print(f\"Vector dimensions: {len(vectors2[0])}\")\n",
    "print(f\"Vectorization results match (first 20): {vectors1 == vectors2[:20]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. NumPy Performance Optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"=== NumPy Performance Optimizations ===\")\n",
    "\n",
    "# Vectorization vs loops\n",
    "@benchmark\n",
    "def compute_distances_loop(points1, points2):\n",
    "    \"\"\"Compute distances using Python loops.\"\"\"\n",
    "    distances = []\n",
    "    for p1 in points1:\n",
    "        row_distances = []\n",
    "        for p2 in points2:\n",
    "            # Euclidean distance\n",
    "            dist = ((p1[0] - p2[0])**2 + (p1[1] - p2[1])**2)**0.5\n",
    "            row_distances.append(dist)\n",
    "        distances.append(row_distances)\n",
    "    return distances\n",
    "\n",
    "@benchmark\n",
    "def compute_distances_numpy(points1, points2):\n",
    "    \"\"\"Compute distances using NumPy vectorization.\"\"\"\n",
    "    # Convert to NumPy arrays if needed\n",
    "    p1 = np.array(points1)\n",
    "    p2 = np.array(points2)\n",
    "    \n",
    "    # Vectorized distance computation\n",
    "    # Broadcasting to compute all pairwise distances\n",
    "    diff = p1[:, np.newaxis, :] - p2[np.newaxis, :, :]\n",
    "    distances = np.sqrt(np.sum(diff**2, axis=2))\n",
    "    \n",
    "    return distances.tolist()\n",
    "\n",
    "# Generate test data\n",
    "np.random.seed(42)\n",
    "points1 = np.random.rand(100, 2).tolist()\n",
    "points2 = np.random.rand(100, 2).tolist()\n",
    "\n",
    "print(f\"Computing distances between {len(points1)} and {len(points2)} points\")\n",
    "\n",
    "distances1 = compute_distances_loop(points1[:20], points2[:20])  # Smaller for loop version\n",
    "distances2 = compute_distances_numpy(points1, points2)\n",
    "\n",
    "# Check if results are approximately equal (first 20x20)\n",
    "diff = np.max(np.abs(np.array(distances1) - np.array(distances2[:20])[:, :20]))\n",
    "print(f\"Maximum difference: {diff:.10f}\")\n",
    "\n",
    "# Memory-efficient operations\n",
    "print(\"\\n=== Memory-Efficient NumPy Operations ===\")\n",
    "\n",
    "@benchmark\n",
    "def matrix_operations_copy(matrix):\n",
    "    \"\"\"Matrix operations that create copies.\"\"\"\n",
    "    result = matrix + 1\n",
    "    result = result * 2\n",
    "    result = result - 0.5\n",
    "    return result\n",
    "\n",
    "@benchmark \n",
    "def matrix_operations_inplace(matrix):\n",
    "    \"\"\"Memory-efficient in-place operations.\"\"\"\n",
    "    result = matrix.copy()  # Only one copy\n",
    "    result += 1\n",
    "    result *= 2\n",
    "    result -= 0.5\n",
    "    return result\n",
    "\n",
    "# Test with large matrix\n",
    "large_matrix = np.random.rand(1000, 1000)\n",
    "\n",
    "print(f\"Matrix shape: {large_matrix.shape}\")\n",
    "print(f\"Matrix memory usage: {large_matrix.nbytes / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "result1 = matrix_operations_copy(large_matrix)\n",
    "result2 = matrix_operations_inplace(large_matrix)\n",
    "\n",
    "print(f\"Results are equal: {np.allclose(result1, result2)}\")\n",
    "\n",
    "# Data type optimization\n",
    "print(\"\\n=== Data Type Optimization ===\")\n",
    "\n",
    "def compare_dtypes():\n",
    "    \"\"\"Compare memory usage of different data types.\"\"\"\n",
    "    size = 1000000\n",
    "    \n",
    "    arrays = {\n",
    "        'float64': np.ones(size, dtype=np.float64),\n",
    "        'float32': np.ones(size, dtype=np.float32),\n",
    "        'int64': np.ones(size, dtype=np.int64),\n",
    "        'int32': np.ones(size, dtype=np.int32),\n",
    "        'int16': np.ones(size, dtype=np.int16),\n",
    "        'int8': np.ones(size, dtype=np.int8),\n",
    "    }\n",
    "    \n",
    "    print(f\"Memory usage for {size} elements:\")\n",
    "    for dtype_name, arr in arrays.items():\n",
    "        memory_mb = arr.nbytes / 1024 / 1024\n",
    "        print(f\"{dtype_name:8}: {memory_mb:6.2f} MB ({arr.dtype.itemsize} bytes per element)\")\n",
    "\n",
    "compare_dtypes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Garbage Collection and Memory Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import weakref\n",
    "\n",
    "print(\"=== Garbage Collection Analysis ===\")\n",
    "\n",
    "def analyze_gc():\n",
    "    \"\"\"Analyze garbage collection behavior.\"\"\"\n",
    "    print(f\"Garbage collection enabled: {gc.isenabled()}\")\n",
    "    print(f\"Current thresholds: {gc.get_threshold()}\")\n",
    "    print(f\"Collection counts: {gc.get_count()}\")\n",
    "    \n",
    "    # Get current stats\n",
    "    stats_before = gc.get_stats()\n",
    "    print(f\"\\nGC stats before: {len(stats_before)} generations\")\n",
    "    \n",
    "    # Create objects that might form cycles\n",
    "    objects = []\n",
    "    for i in range(1000):\n",
    "        obj = {'id': i, 'data': list(range(100))}\n",
    "        obj['self_ref'] = obj  # Create circular reference\n",
    "        objects.append(obj)\n",
    "    \n",
    "    print(f\"Created {len(objects)} objects with circular references\")\n",
    "    print(f\"Collection counts after creation: {gc.get_count()}\")\n",
    "    \n",
    "    # Force garbage collection\n",
    "    collected = gc.collect()\n",
    "    print(f\"Objects collected by gc.collect(): {collected}\")\n",
    "    print(f\"Collection counts after gc.collect(): {gc.get_count()}\")\n",
    "    \n",
    "    # Clear references and collect again\n",
    "    objects.clear()\n",
    "    collected = gc.collect()\n",
    "    print(f\"Objects collected after clearing references: {collected}\")\n",
    "\n",
    "analyze_gc()\n",
    "\n",
    "# Weak references to avoid memory leaks\n",
    "print(\"\\n=== Weak References Demo ===\")\n",
    "\n",
    "class DataProcessor:\n",
    "    \"\"\"Example class for weak reference demo.\"\"\"\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.data = list(range(1000))  # Some data\n",
    "    \n",
    "    def __del__(self):\n",
    "        print(f\"DataProcessor {self.name} is being deleted\")\n",
    "\n",
    "def weak_reference_demo():\n",
    "    \"\"\"Demonstrate weak references.\"\"\"\n",
    "    # Create object\n",
    "    processor = DataProcessor(\"main\")\n",
    "    \n",
    "    # Create weak reference\n",
    "    weak_ref = weakref.ref(processor)\n",
    "    print(f\"Weak reference created: {weak_ref}\")\n",
    "    print(f\"Object accessible via weak ref: {weak_ref() is not None}\")\n",
    "    \n",
    "    # Delete original reference\n",
    "    del processor\n",
    "    \n",
    "    # Force garbage collection\n",
    "    gc.collect()\n",
    "    \n",
    "    print(f\"Object accessible after deletion: {weak_ref() is not None}\")\n",
    "    if weak_ref() is None:\n",
    "        print(\"Weak reference is now dead\")\n",
    "\n",
    "weak_reference_demo()\n",
    "\n",
    "# Context managers for resource management\n",
    "print(\"\\n=== Context Managers for Resource Management ===\")\n",
    "\n",
    "class MemoryManagedProcessor:\n",
    "    \"\"\"Context manager for automatic resource cleanup.\"\"\"\n",
    "    \n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "        self.data = None\n",
    "    \n",
    "    def __enter__(self):\n",
    "        print(f\"Allocating {self.size} elements\")\n",
    "        self.data = list(range(self.size))\n",
    "        initial_memory = get_memory_usage()\n",
    "        print(f\"Memory after allocation: {initial_memory:.2f} MB\")\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        print(\"Cleaning up resources\")\n",
    "        self.data = None\n",
    "        gc.collect()\n",
    "        final_memory = get_memory_usage()\n",
    "        print(f\"Memory after cleanup: {final_memory:.2f} MB\")\n",
    "    \n",
    "    def process(self):\n",
    "        \"\"\"Simulate some processing.\"\"\"\n",
    "        if self.data:\n",
    "            return sum(x**2 for x in self.data[:1000])  # Process first 1000 elements\n",
    "        return 0\n",
    "\n",
    "# Use context manager\n",
    "print(f\"Initial memory: {get_memory_usage():.2f} MB\")\n",
    "\n",
    "with MemoryManagedProcessor(100000) as processor:\n",
    "    result = processor.process()\n",
    "    print(f\"Processing result: {result}\")\n",
    "\n",
    "print(f\"Final memory: {get_memory_usage():.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Practical Performance Optimization Tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Practical Performance Tips ===\")\n",
    "\n",
    "# 1. Use slots for memory efficiency\n",
    "class RegularClass:\n",
    "    \"\"\"Regular class without slots.\"\"\"\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "class SlottedClass:\n",
    "    \"\"\"Memory-efficient class with slots.\"\"\"\n",
    "    __slots__ = ['x', 'y']\n",
    "    \n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "def compare_class_memory():\n",
    "    \"\"\"Compare memory usage of regular vs slotted classes.\"\"\"\n",
    "    n = 10000\n",
    "    \n",
    "    # Create instances\n",
    "    regular_objects = [RegularClass(i, i*2) for i in range(n)]\n",
    "    slotted_objects = [SlottedClass(i, i*2) for i in range(n)]\n",
    "    \n",
    "    # Measure memory usage\n",
    "    regular_size = sum(sys.getsizeof(obj) + sys.getsizeof(obj.__dict__) for obj in regular_objects)\n",
    "    slotted_size = sum(sys.getsizeof(obj) for obj in slotted_objects)\n",
    "    \n",
    "    print(f\"Memory comparison for {n} objects:\")\n",
    "    print(f\"Regular class: {regular_size / 1024:.2f} KB\")\n",
    "    print(f\"Slotted class: {slotted_size / 1024:.2f} KB\")\n",
    "    print(f\"Memory savings: {(regular_size - slotted_size) / regular_size * 100:.1f}%\")\n",
    "\n",
    "compare_class_memory()\n",
    "\n",
    "# 2. Efficient string operations\n",
    "print(\"\\n=== String Operation Optimizations ===\")\n",
    "\n",
    "@benchmark\n",
    "def string_formatting_old(items):\n",
    "    \"\"\"Old-style string formatting.\"\"\"\n",
    "    return ['Item: %s, Value: %d' % (item['name'], item['value']) for item in items]\n",
    "\n",
    "@benchmark\n",
    "def string_formatting_new(items):\n",
    "    \"\"\"New-style string formatting.\"\"\"\n",
    "    return ['Item: {}, Value: {}'.format(item['name'], item['value']) for item in items]\n",
    "\n",
    "@benchmark\n",
    "def string_formatting_fstring(items):\n",
    "    \"\"\"f-string formatting (fastest).\"\"\"\n",
    "    return [f\"Item: {item['name']}, Value: {item['value']}\" for item in items]\n",
    "\n",
    "# Test string formatting\n",
    "test_items = [{'name': f'item_{i}', 'value': i} for i in range(10000)]\n",
    "\n",
    "result1 = string_formatting_old(test_items)\n",
    "result2 = string_formatting_new(test_items)\n",
    "result3 = string_formatting_fstring(test_items)\n",
    "\n",
    "print(f\"All string formatting results equal: {result1 == result2 == result3}\")\n",
    "\n",
    "# 3. Set operations vs list operations\n",
    "print(\"\\n=== Set vs List Operations ===\")\n",
    "\n",
    "large_list1 = list(range(10000))\n",
    "large_list2 = list(range(5000, 15000))\n",
    "large_set1 = set(large_list1)\n",
    "large_set2 = set(large_list2)\n",
    "\n",
    "@benchmark\n",
    "def find_intersection_list(list1, list2):\n",
    "    \"\"\"Find intersection using lists.\"\"\"\n",
    "    return [x for x in list1 if x in list2]\n",
    "\n",
    "@benchmark\n",
    "def find_intersection_set(set1, set2):\n",
    "    \"\"\"Find intersection using sets.\"\"\"\n",
    "    return list(set1 & set2)\n",
    "\n",
    "print(f\"List sizes: {len(large_list1)}, {len(large_list2)}\")\n",
    "\n",
    "intersection1 = find_intersection_list(large_list1[:1000], large_list2)  # Smaller for list version\n",
    "intersection2 = find_intersection_set(large_set1, large_set2)\n",
    "\n",
    "print(f\"Intersection sizes: {len(intersection1)}, {len(intersection2)}\")\n",
    "\n",
    "# 4. Generator expressions for memory efficiency\n",
    "print(\"\\n=== Generator Memory Efficiency ===\")\n",
    "\n",
    "def memory_usage_comparison():\n",
    "    \"\"\"Compare memory usage of list vs generator.\"\"\"\n",
    "    n = 1000000\n",
    "    \n",
    "    # List comprehension - all in memory\n",
    "    print(f\"Creating list of {n} elements...\")\n",
    "    initial_memory = get_memory_usage()\n",
    "    \n",
    "    large_list = [x**2 for x in range(n)]\n",
    "    list_memory = get_memory_usage()\n",
    "    \n",
    "    print(f\"List memory increase: {list_memory - initial_memory:.2f} MB\")\n",
    "    \n",
    "    # Clear the list\n",
    "    del large_list\n",
    "    gc.collect()\n",
    "    \n",
    "    # Generator expression - lazy evaluation\n",
    "    print(f\"Creating generator for {n} elements...\")\n",
    "    generator_initial = get_memory_usage()\n",
    "    \n",
    "    large_generator = (x**2 for x in range(n))\n",
    "    generator_memory = get_memory_usage()\n",
    "    \n",
    "    print(f\"Generator memory increase: {generator_memory - generator_initial:.2f} MB\")\n",
    "    \n",
    "    # Consume first 10 elements from generator\n",
    "    first_ten = [next(large_generator) for _ in range(10)]\n",
    "    print(f\"First 10 elements: {first_ten}\")\n",
    "\n",
    "memory_usage_comparison()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Performance Optimization Summary:\")\n",
    "print(\"1. Use built-in functions when possible\")\n",
    "print(\"2. Prefer list comprehensions over loops\")\n",
    "print(\"3. Use generators for large datasets\")\n",
    "print(\"4. Choose appropriate data types\")\n",
    "print(\"5. Use __slots__ for memory-efficient classes\")\n",
    "print(\"6. Leverage NumPy for numerical computations\")\n",
    "print(\"7. Profile your code to identify bottlenecks\")\n",
    "print(\"8. Use caching for expensive computations\")\n",
    "print(\"9. Manage memory with context managers\")\n",
    "print(\"10. Monitor garbage collection behavior\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Real-world ML Performance Case Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== ML Performance Case Study: Text Classification Pipeline ===\")\n",
    "\n",
    "# Simulate a complete text classification pipeline with performance monitoring\n",
    "\n",
    "class TextClassificationPipeline:\n",
    "    \"\"\"Optimized text classification pipeline.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.vocab = None\n",
    "        self.word_to_idx = None\n",
    "        self.model_weights = None\n",
    "        \n",
    "        # Pre-compile regex patterns for efficiency\n",
    "        self.punct_pattern = re.compile(r'[^\\w\\s]')\n",
    "        self.whitespace_pattern = re.compile(r'\\s+')\n",
    "    \n",
    "    @benchmark\n",
    "    def preprocess_batch(self, texts):\n",
    "        \"\"\"Efficiently preprocess a batch of texts.\"\"\"\n",
    "        processed = []\n",
    "        \n",
    "        for text in texts:\n",
    "            # Single-pass preprocessing\n",
    "            text = self.punct_pattern.sub(' ', text.lower())\n",
    "            text = self.whitespace_pattern.sub(' ', text).strip()\n",
    "            \n",
    "            # Tokenize and filter\n",
    "            tokens = [word for word in text.split() if len(word) > 2]\n",
    "            processed.append(tokens)\n",
    "        \n",
    "        return processed\n",
    "    \n",
    "    @benchmark\n",
    "    def build_vocabulary(self, processed_texts, max_vocab=5000):\n",
    "        \"\"\"Build vocabulary efficiently using Counter.\"\"\"\n",
    "        # Count all words\n",
    "        word_counts = Counter()\n",
    "        for tokens in processed_texts:\n",
    "            word_counts.update(tokens)\n",
    "        \n",
    "        # Get most common words\n",
    "        self.vocab = [word for word, _ in word_counts.most_common(max_vocab)]\n",
    "        self.word_to_idx = {word: idx for idx, word in enumerate(self.vocab)}\n",
    "        \n",
    "        return len(self.vocab)\n",
    "    \n",
    "    @benchmark\n",
    "    def vectorize_batch(self, processed_texts):\n",
    "        \"\"\"Efficiently vectorize texts using NumPy.\"\"\"\n",
    "        if not self.vocab:\n",
    "            raise ValueError(\"Must build vocabulary first\")\n",
    "        \n",
    "        # Pre-allocate NumPy array for efficiency\n",
    "        n_texts = len(processed_texts)\n",
    "        n_features = len(self.vocab)\n",
    "        vectors = np.zeros((n_texts, n_features), dtype=np.float32)\n",
    "        \n",
    "        # Vectorize efficiently\n",
    "        for i, tokens in enumerate(processed_texts):\n",
    "            token_counts = Counter(tokens)\n",
    "            for word, count in token_counts.items():\n",
    "                if word in self.word_to_idx:\n",
    "                    idx = self.word_to_idx[word]\n",
    "                    vectors[i, idx] = count\n",
    "        \n",
    "        return vectors\n",
    "    \n",
    "    @benchmark\n",
    "    def train_model(self, X, y, learning_rate=0.01, epochs=10):\n",
    "        \"\"\"Simple logistic regression training with NumPy.\"\"\"\n",
    "        n_features = X.shape[1]\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.model_weights = np.random.normal(0, 0.01, n_features).astype(np.float32)\n",
    "        \n",
    "        # Training loop with vectorized operations\n",
    "        for epoch in range(epochs):\n",
    "            # Forward pass\n",
    "            predictions = np.dot(X, self.model_weights)\n",
    "            probabilities = 1 / (1 + np.exp(-predictions))\n",
    "            \n",
    "            # Backward pass\n",
    "            errors = y - probabilities\n",
    "            gradient = np.dot(X.T, errors) / len(y)\n",
    "            \n",
    "            # Update weights\n",
    "            self.model_weights += learning_rate * gradient\n",
    "            \n",
    "            if (epoch + 1) % 5 == 0:\n",
    "                loss = -np.mean(y * np.log(probabilities + 1e-15) + \n",
    "                              (1 - y) * np.log(1 - probabilities + 1e-15))\n",
    "                print(f\"  Epoch {epoch + 1}: Loss = {loss:.4f}\")\n",
    "    \n",
    "    @benchmark\n",
    "    def predict_batch(self, X):\n",
    "        \"\"\"Efficient batch prediction.\"\"\"\n",
    "        if self.model_weights is None:\n",
    "            raise ValueError(\"Model must be trained first\")\n",
    "        \n",
    "        predictions = np.dot(X, self.model_weights)\n",
    "        probabilities = 1 / (1 + np.exp(-predictions))\n",
    "        return (probabilities > 0.5).astype(int)\n",
    "\n",
    "# Generate sample data for the case study\n",
    "def generate_sample_data(n_samples=5000):\n",
    "    \"\"\"Generate sample text data for classification.\"\"\"\n",
    "    positive_words = ['excellent', 'amazing', 'fantastic', 'great', 'wonderful', 'perfect']\n",
    "    negative_words = ['terrible', 'awful', 'horrible', 'bad', 'worst', 'disappointing']\n",
    "    neutral_words = ['okay', 'average', 'normal', 'standard', 'typical', 'regular']\n",
    "    \n",
    "    texts = []\n",
    "    labels = []\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        if i % 3 == 0:  # Positive\n",
    "            words = np.random.choice(positive_words, size=np.random.randint(3, 8))\n",
    "            label = 1\n",
    "        elif i % 3 == 1:  # Negative  \n",
    "            words = np.random.choice(negative_words, size=np.random.randint(3, 8))\n",
    "            label = 0\n",
    "        else:  # Mixed\n",
    "            words = np.random.choice(neutral_words + positive_words + negative_words, \n",
    "                                   size=np.random.randint(5, 12))\n",
    "            label = np.random.randint(0, 2)\n",
    "        \n",
    "        text = ' '.join(words) + ' product review text here'\n",
    "        texts.append(text)\n",
    "        labels.append(label)\n",
    "    \n",
    "    return texts, labels\n",
    "\n",
    "# Run the complete pipeline\n",
    "print(\"Generating sample data...\")\n",
    "texts, labels = generate_sample_data(3000)\n",
    "labels = np.array(labels, dtype=np.float32)\n",
    "\n",
    "print(f\"Dataset: {len(texts)} texts, {np.mean(labels):.2f} positive ratio\")\n",
    "\n",
    "# Initialize pipeline\n",
    "pipeline = TextClassificationPipeline()\n",
    "\n",
    "print(f\"\\nInitial memory usage: {get_memory_usage():.2f} MB\")\n",
    "\n",
    "# Step 1: Preprocess\n",
    "print(\"\\n1. Preprocessing texts...\")\n",
    "processed_texts = pipeline.preprocess_batch(texts)\n",
    "print(f\"Memory after preprocessing: {get_memory_usage():.2f} MB\")\n",
    "\n",
    "# Step 2: Build vocabulary\n",
    "print(\"\\n2. Building vocabulary...\")\n",
    "vocab_size = pipeline.build_vocabulary(processed_texts, max_vocab=1000)\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"Memory after vocabulary: {get_memory_usage():.2f} MB\")\n",
    "\n",
    "# Step 3: Vectorize\n",
    "print(\"\\n3. Vectorizing texts...\")\n",
    "X = pipeline.vectorize_batch(processed_texts)\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Feature matrix memory: {X.nbytes / 1024 / 1024:.2f} MB\")\n",
    "print(f\"Memory after vectorization: {get_memory_usage():.2f} MB\")\n",
    "\n",
    "# Step 4: Train model\n",
    "print(\"\\n4. Training model...\")\n",
    "pipeline.train_model(X, labels, learning_rate=0.1, epochs=20)\n",
    "print(f\"Memory after training: {get_memory_usage():.2f} MB\")\n",
    "\n",
    "# Step 5: Evaluate\n",
    "print(\"\\n5. Making predictions...\")\n",
    "predictions = pipeline.predict_batch(X)\n",
    "accuracy = np.mean(predictions == labels)\n",
    "print(f\"Training accuracy: {accuracy:.3f}\")\n",
    "print(f\"Final memory usage: {get_memory_usage():.2f} MB\")\n",
    "\n",
    "# Performance summary\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Pipeline Performance Summary:\")\n",
    "print(f\"• Processed {len(texts)} texts\")\n",
    "print(f\"• Built vocabulary of {vocab_size} words\")\n",
    "print(f\"• Created {X.shape[0]}x{X.shape[1]} feature matrix\")\n",
    "print(f\"• Achieved {accuracy:.1%} training accuracy\")\n",
    "print(f\"• Memory-efficient operations throughout\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Exercises\n",
    "\n",
    "### Exercise 1: Memory Profiling\n",
    "Profile the memory usage of a word embedding loading function and optimize it.\n",
    "\n",
    "### Exercise 2: Algorithm Optimization\n",
    "Optimize a cosine similarity calculation between document vectors.\n",
    "\n",
    "### Exercise 3: Data Structure Choice\n",
    "Compare performance of different data structures for storing and querying large vocabularies.\n",
    "\n",
    "### Exercise 4: Batch Processing\n",
    "Implement efficient batch processing for a text classification pipeline.\n",
    "\n",
    "### Exercise 5: Memory Leak Detection\n",
    "Identify and fix memory leaks in a streaming text processing system.\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Memory Management**: Understanding Python's memory model is crucial for ML/NLP\n",
    "2. **Profiling**: Use tools like `tracemalloc`, `cProfile`, and `memory_profiler`\n",
    "3. **Data Structures**: Choose the right data structure for your use case\n",
    "4. **Vectorization**: Use NumPy for numerical operations\n",
    "5. **Generators**: Use generators for memory-efficient processing of large datasets\n",
    "6. **Optimization**: Profile first, then optimize bottlenecks\n",
    "7. **Resource Management**: Use context managers and proper cleanup\n",
    "8. **Caching**: Use `@lru_cache` for expensive computations\n",
    "\n",
    "These performance and memory management skills are essential for building scalable ML and NLP applications!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}